{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TFX Basic Shared Libraries","text":"<p>TFX Basic Shared Libraries (<code>tfx_bsl</code>) contains libraries shared by many TensorFlow eXtended (TFX) components.</p> <p>Only symbols exported by sub-modules under <code>tfx_bsl/public</code> are intended for direct use by TFX users, including by standalone TFX library (e.g. TFDV, TFMA, TFT) users, TFX pipeline authors and TFX component authors. Those APIs will become stable and follow semantic versioning once <code>tfx_bsl</code> goes beyond <code>1.0</code>.</p> <p>APIs under other directories should be considered internal to TFX (and therefore there is no backward or forward compatibility guarantee for them).</p> <p>Each minor version of a TFX library or TFX itself, if it needs to depend on <code>tfx_bsl</code>, will depend on a specific minor version of it (e.g. <code>tensorflow_data_validation</code> 0.14.* will depend on, and only work with, <code>tfx_bsl</code> 0.14.*)</p>"},{"location":"#installing-from-pypi","title":"Installing from PyPI","text":"<p><code>tfx_bsl</code> is available as a PyPI package.</p> <pre><code>pip install tfx-bsl\n</code></pre>"},{"location":"#nightly-packages","title":"Nightly Packages","text":"<p>TFX-BSL also hosts nightly packages at https://pypi-nightly.tensorflow.org on Google Cloud. To install the latest nightly package, please use the following command:</p> <pre><code>pip install --extra-index-url https://pypi-nightly.tensorflow.org/simple tfx-bsl\n</code></pre> <p>This will install the nightly packages for the major dependencies of TFX-BSL such as TensorFlow Metadata (TFMD).</p> <p>However it is a dependency of many TFX components and usually as a user you don't need to install it directly.</p>"},{"location":"#build-from-source","title":"Build from source","text":""},{"location":"#1-prerequisites","title":"1. Prerequisites","text":""},{"location":"#install-numpy","title":"Install NumPy","text":"<p>If NumPy is not installed on your system, install it now by following these directions.</p>"},{"location":"#install-bazel","title":"Install Bazel","text":"<p>If Bazel is not installed on your system, install it now by following these directions.</p>"},{"location":"#install-cibuildwheel","title":"Install cibuildwheel","text":"<p>If you do not already have cibuildwheel installed on your system, you an install it using these directions.</p>"},{"location":"#2-clone-the-tfx_bsl-repository","title":"2. Clone the <code>tfx_bsl</code> repository","text":"<pre><code>git clone https://github.com/tensorflow/tfx-bsl\ncd tfx-bsl\n</code></pre> <p>Note that these instructions will install the latest master branch of <code>tfx_bsl</code> If you want to install a specific branch (such as a release branch), pass <code>-b &lt;branchname&gt;</code> to the <code>git clone</code> command.</p>"},{"location":"#3-build-the-pip-package","title":"3. Build the pip package","text":"<p><code>tfx_bsl</code> wheel is Python version dependent -- to build the pip package that works for a specific Python version, use that Python binary to run: <pre><code>cibuildwheel\n</code></pre></p> <p>You can find the generated <code>.whl</code> file in the <code>dist</code> subdirectory.</p>"},{"location":"#4-install-the-pip-package","title":"4. Install the pip package","text":"<pre><code>pip install dist/*.whl\n</code></pre>"},{"location":"#supported-platforms","title":"Supported platforms","text":"<p><code>tfx_bsl</code> is tested on the following 64-bit operating systems:</p> <ul> <li>macOS 10.12.6 (Sierra) or later.</li> <li>Ubuntu 20.04 or later.</li> </ul>"},{"location":"#compatible-versions","title":"Compatible versions","text":"<p>The following table is the <code>tfx_bsl</code> package versions that are compatible with each other. This is determined by our testing framework, but other untested combinations may also work.</p> tfx-bsl apache-beam[gcp] pyarrow tensorflow tensorflow-metadata tensorflow-serving-api GitHub master 2.64.0 10.0.1 nightly (2.x) 1.17.1 2.17.1 1.17.1 2.64.0 10.0.1 2.17 1.17.1 2.17.1 1.17.0 2.64.0 10.0.1 2.17 1.17.1 2.17.1 1.16.1 2.59.0 10.0.1 2.16 1.16.1 2.16.1 1.16.0 2.59.0 10.0.1 2.16 1.16.0 2.16.1 1.15.1 2.47.0 10.0.0 2.15 1.15.0 2.15.1 1.15.0 2.47.0 10.0.0 2.15 1.15.0 2.15.1 1.14.0 2.47.0 10.0.0 2.13 1.14.0 2.13.0 1.13.0 2.40.0 6.0.0 2.12 1.13.1 2.9.0 1.12.0 2.40.0 6.0.0 2.11 1.12.0 2.9.0 1.11.0 2.40.0 6.0.0 1.15 / 2.10 1.11.0 2.9.0 1.10.0 2.40.0 6.0.0 1.15 / 2.9 1.10.0 2.9.0 1.9.0 2.38.0 5.0.0 1.15 / 2.9 1.9.0 2.9.0 1.8.0 2.38.0 5.0.0 1.15 / 2.8 1.8.0 2.8.0 1.7.0 2.36.0 5.0.0 1.15 / 2.8 1.7.0 2.8.0 1.6.0 2.35.0 5.0.0 1.15 / 2.7 1.6.0 2.7.0 1.5.0 2.34.0 5.0.0 1.15 / 2.7 1.5.0 2.7.0 1.4.0 2.31.0 5.0.0 1.15 / 2.6 1.4.0 2.6.0 1.3.0 2.31.0 2.0.0 1.15 / 2.6 1.2.0 2.6.0 1.2.0 2.31.0 2.0.0 1.15 / 2.5 1.2.0 2.5.1 1.1.0 2.29.0 2.0.0 1.15 / 2.5 1.1.0 2.5.1 1.0.0 2.29.0 2.0.0 1.15 / 2.5 1.0.0 2.5.1 0.30.0 2.28.0 2.0.0 1.15 / 2.4 0.30.0 2.4.0 0.29.0 2.28.0 2.0.0 1.15 / 2.4 0.29.0 2.4.0 0.28.0 2.28.0 2.0.0 1.15 / 2.4 0.28.0 2.4.0 0.27.1 2.27.0 2.0.0 1.15 / 2.4 0.27.0 2.4.0 0.27.0 2.27.0 2.0.0 1.15 / 2.4 0.27.0 2.4.0 0.26.1 2.25.0 0.17.0 1.15 / 2.3 0.27.0 2.3.0 0.26.0 2.25.0 0.17.0 1.15 / 2.3 0.27.0 2.3.0"},{"location":"api_docs/python/beam/","title":"TFX-BSL Public Beam","text":""},{"location":"api_docs/python/beam/#tfx_bsl.public.beam","title":"tfx_bsl.public.beam","text":"<p>Module level imports for tfx_bsl.beam.</p>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam-functions","title":"Functions","text":""},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.RunInference","title":"RunInference","text":"<pre><code>RunInference(\n    examples: PCollection,\n    inference_spec_type: InferenceSpecType,\n    load_override_fn: Optional[LoadOverrideFnType] = None,\n) -&gt; PCollection\n</code></pre> <p>Run inference with a model.</p> There are two types of inference you can perform using this PTransform <ol> <li>In-process inference from a SavedModel instance. Used when   <code>saved_model_spec</code> field is set in <code>inference_spec_type</code>.</li> <li>Remote inference by using a service endpoint. Used when   <code>ai_platform_prediction_model_spec</code> field is set in   <code>inference_spec_type</code>.</li> </ol> <p>examples: A PCollection containing examples of the following possible kinds,     each with their corresponding return type.       - PCollection[Example]                   -&gt; PCollection[PredictionLog]           * Works with Classify, Regress, MultiInference, Predict and             RemotePredict.</p> <pre><code>  - PCollection[SequenceExample]           -&gt; PCollection[PredictionLog]\n      * Works with Predict and (serialized) RemotePredict.\n\n  - PCollection[bytes]                     -&gt; PCollection[PredictionLog]\n      * For serialized Example: Works with Classify, Regress,\n        MultiInference, Predict and RemotePredict.\n      * For everything else: Works with Predict and RemotePredict.\n\n  - PCollection[Tuple[K, Example]]         -&gt; PCollection[\n                                                  Tuple[K, PredictionLog]]\n      * Works with Classify, Regress, MultiInference, Predict and\n        RemotePredict.\n\n  - PCollection[Tuple[K, SequenceExample]] -&gt; PCollection[\n                                                  Tuple[K, PredictionLog]]\n      * Works with Predict and (serialized) RemotePredict.\n\n  - PCollection[Tuple[K, bytes]]           -&gt; PCollection[\n                                                  Tuple[K, PredictionLog]]\n      * For serialized Example: Works with Classify, Regress,\n        MultiInference, Predict and RemotePredict.\n      * For everything else: Works with Predict and RemotePredict.\n</code></pre> <p>inference_spec_type: Model inference endpoint.   load_override_fn: Optional function taking a model path and sequence of     tags, and returning a tf SavedModel. The loaded model must be equivalent     in interface to the model that would otherwise be loaded. It is up to the     caller to ensure compatibility. This argument is experimental and subject     to change.</p> <p>A PCollection (possibly keyed) containing prediction logs.</p> Source code in <code>tfx_bsl/public/beam/run_inference.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(_MaybeKeyedInput)\n@beam.typehints.with_output_types(Union[_OUTPUT_TYPE, Tuple[_K, _OUTPUT_TYPE]])\ndef RunInference(\n    examples: beam.pvalue.PCollection,\n    inference_spec_type: model_spec_pb2.InferenceSpecType,\n    load_override_fn: Optional[run_inference.LoadOverrideFnType] = None,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Run inference with a model.\n\n    There are two types of inference you can perform using this PTransform:\n      1. In-process inference from a SavedModel instance. Used when\n        `saved_model_spec` field is set in `inference_spec_type`.\n      2. Remote inference by using a service endpoint. Used when\n        `ai_platform_prediction_model_spec` field is set in\n        `inference_spec_type`.\n\n    Args:\n    ----\n      examples: A PCollection containing examples of the following possible kinds,\n        each with their corresponding return type.\n          - PCollection[Example]                   -&gt; PCollection[PredictionLog]\n              * Works with Classify, Regress, MultiInference, Predict and\n                RemotePredict.\n\n          - PCollection[SequenceExample]           -&gt; PCollection[PredictionLog]\n              * Works with Predict and (serialized) RemotePredict.\n\n          - PCollection[bytes]                     -&gt; PCollection[PredictionLog]\n              * For serialized Example: Works with Classify, Regress,\n                MultiInference, Predict and RemotePredict.\n              * For everything else: Works with Predict and RemotePredict.\n\n          - PCollection[Tuple[K, Example]]         -&gt; PCollection[\n                                                          Tuple[K, PredictionLog]]\n              * Works with Classify, Regress, MultiInference, Predict and\n                RemotePredict.\n\n          - PCollection[Tuple[K, SequenceExample]] -&gt; PCollection[\n                                                          Tuple[K, PredictionLog]]\n              * Works with Predict and (serialized) RemotePredict.\n\n          - PCollection[Tuple[K, bytes]]           -&gt; PCollection[\n                                                          Tuple[K, PredictionLog]]\n              * For serialized Example: Works with Classify, Regress,\n                MultiInference, Predict and RemotePredict.\n              * For everything else: Works with Predict and RemotePredict.\n\n      inference_spec_type: Model inference endpoint.\n      load_override_fn: Optional function taking a model path and sequence of\n        tags, and returning a tf SavedModel. The loaded model must be equivalent\n        in interface to the model that would otherwise be loaded. It is up to the\n        caller to ensure compatibility. This argument is experimental and subject\n        to change.\n\n    Returns:\n    -------\n      A PCollection (possibly keyed) containing prediction logs.\n    \"\"\"\n    return examples | \"RunInferenceImpl\" &gt;&gt; run_inference.RunInferenceImpl(\n        inference_spec_type, load_override_fn\n    )\n</code></pre>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam-modules","title":"Modules","text":""},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference","title":"run_inference","text":"<p>Public API of batch inference.</p>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference-functions","title":"Functions","text":""},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference.CreateModelHandler","title":"CreateModelHandler","text":"<pre><code>CreateModelHandler(\n    inference_spec_type: InferenceSpecType,\n) -&gt; ModelHandler\n</code></pre> <p>Creates a Beam ModelHandler based on the inference spec type.</p> There are two model handlers <ol> <li>In-process inference from a SavedModel instance. Used when   <code>saved_model_spec</code> field is set in <code>inference_spec_type</code>.</li> <li>Remote inference by using a service endpoint. Used when   <code>ai_platform_prediction_model_spec</code> field is set in   <code>inference_spec_type</code>.</li> </ol> <p>Example Usage:</p> <pre><code>from apache_beam.ml.inference import base\n\ntf_handler = CreateModelHandler(inference_spec_type)\n# unkeyed\nbase.RunInference(tf_handler)\n\n# keyed\nbase.RunInference(base.KeyedModelHandler(tf_handler))\n</code></pre> <p>inference_spec_type: Model inference endpoint.</p> <p>A Beam RunInference ModelHandler for TensorFlow</p> Source code in <code>tfx_bsl/public/beam/run_inference.py</code> <pre><code>def CreateModelHandler(\n    inference_spec_type: model_spec_pb2.InferenceSpecType,\n) -&gt; ModelHandler:\n    \"\"\"Creates a Beam ModelHandler based on the inference spec type.\n\n    There are two model handlers:\n      1. In-process inference from a SavedModel instance. Used when\n        `saved_model_spec` field is set in `inference_spec_type`.\n      2. Remote inference by using a service endpoint. Used when\n        `ai_platform_prediction_model_spec` field is set in\n        `inference_spec_type`.\n\n    Example Usage:\n\n      ```\n      from apache_beam.ml.inference import base\n\n      tf_handler = CreateModelHandler(inference_spec_type)\n      # unkeyed\n      base.RunInference(tf_handler)\n\n      # keyed\n      base.RunInference(base.KeyedModelHandler(tf_handler))\n      ```\n\n    Args:\n    ----\n      inference_spec_type: Model inference endpoint.\n\n    Returns:\n    -------\n      A Beam RunInference ModelHandler for TensorFlow\n    \"\"\"\n    return run_inference.create_model_handler(inference_spec_type, None, None)\n</code></pre>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference.RunInference","title":"RunInference","text":"<pre><code>RunInference(\n    examples: PCollection,\n    inference_spec_type: InferenceSpecType,\n    load_override_fn: Optional[LoadOverrideFnType] = None,\n) -&gt; PCollection\n</code></pre> <p>Run inference with a model.</p> There are two types of inference you can perform using this PTransform <ol> <li>In-process inference from a SavedModel instance. Used when   <code>saved_model_spec</code> field is set in <code>inference_spec_type</code>.</li> <li>Remote inference by using a service endpoint. Used when   <code>ai_platform_prediction_model_spec</code> field is set in   <code>inference_spec_type</code>.</li> </ol> <p>examples: A PCollection containing examples of the following possible kinds,     each with their corresponding return type.       - PCollection[Example]                   -&gt; PCollection[PredictionLog]           * Works with Classify, Regress, MultiInference, Predict and             RemotePredict.</p> <pre><code>  - PCollection[SequenceExample]           -&gt; PCollection[PredictionLog]\n      * Works with Predict and (serialized) RemotePredict.\n\n  - PCollection[bytes]                     -&gt; PCollection[PredictionLog]\n      * For serialized Example: Works with Classify, Regress,\n        MultiInference, Predict and RemotePredict.\n      * For everything else: Works with Predict and RemotePredict.\n\n  - PCollection[Tuple[K, Example]]         -&gt; PCollection[\n                                                  Tuple[K, PredictionLog]]\n      * Works with Classify, Regress, MultiInference, Predict and\n        RemotePredict.\n\n  - PCollection[Tuple[K, SequenceExample]] -&gt; PCollection[\n                                                  Tuple[K, PredictionLog]]\n      * Works with Predict and (serialized) RemotePredict.\n\n  - PCollection[Tuple[K, bytes]]           -&gt; PCollection[\n                                                  Tuple[K, PredictionLog]]\n      * For serialized Example: Works with Classify, Regress,\n        MultiInference, Predict and RemotePredict.\n      * For everything else: Works with Predict and RemotePredict.\n</code></pre> <p>inference_spec_type: Model inference endpoint.   load_override_fn: Optional function taking a model path and sequence of     tags, and returning a tf SavedModel. The loaded model must be equivalent     in interface to the model that would otherwise be loaded. It is up to the     caller to ensure compatibility. This argument is experimental and subject     to change.</p> <p>A PCollection (possibly keyed) containing prediction logs.</p> Source code in <code>tfx_bsl/public/beam/run_inference.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(_MaybeKeyedInput)\n@beam.typehints.with_output_types(Union[_OUTPUT_TYPE, Tuple[_K, _OUTPUT_TYPE]])\ndef RunInference(\n    examples: beam.pvalue.PCollection,\n    inference_spec_type: model_spec_pb2.InferenceSpecType,\n    load_override_fn: Optional[run_inference.LoadOverrideFnType] = None,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Run inference with a model.\n\n    There are two types of inference you can perform using this PTransform:\n      1. In-process inference from a SavedModel instance. Used when\n        `saved_model_spec` field is set in `inference_spec_type`.\n      2. Remote inference by using a service endpoint. Used when\n        `ai_platform_prediction_model_spec` field is set in\n        `inference_spec_type`.\n\n    Args:\n    ----\n      examples: A PCollection containing examples of the following possible kinds,\n        each with their corresponding return type.\n          - PCollection[Example]                   -&gt; PCollection[PredictionLog]\n              * Works with Classify, Regress, MultiInference, Predict and\n                RemotePredict.\n\n          - PCollection[SequenceExample]           -&gt; PCollection[PredictionLog]\n              * Works with Predict and (serialized) RemotePredict.\n\n          - PCollection[bytes]                     -&gt; PCollection[PredictionLog]\n              * For serialized Example: Works with Classify, Regress,\n                MultiInference, Predict and RemotePredict.\n              * For everything else: Works with Predict and RemotePredict.\n\n          - PCollection[Tuple[K, Example]]         -&gt; PCollection[\n                                                          Tuple[K, PredictionLog]]\n              * Works with Classify, Regress, MultiInference, Predict and\n                RemotePredict.\n\n          - PCollection[Tuple[K, SequenceExample]] -&gt; PCollection[\n                                                          Tuple[K, PredictionLog]]\n              * Works with Predict and (serialized) RemotePredict.\n\n          - PCollection[Tuple[K, bytes]]           -&gt; PCollection[\n                                                          Tuple[K, PredictionLog]]\n              * For serialized Example: Works with Classify, Regress,\n                MultiInference, Predict and RemotePredict.\n              * For everything else: Works with Predict and RemotePredict.\n\n      inference_spec_type: Model inference endpoint.\n      load_override_fn: Optional function taking a model path and sequence of\n        tags, and returning a tf SavedModel. The loaded model must be equivalent\n        in interface to the model that would otherwise be loaded. It is up to the\n        caller to ensure compatibility. This argument is experimental and subject\n        to change.\n\n    Returns:\n    -------\n      A PCollection (possibly keyed) containing prediction logs.\n    \"\"\"\n    return examples | \"RunInferenceImpl\" &gt;&gt; run_inference.RunInferenceImpl(\n        inference_spec_type, load_override_fn\n    )\n</code></pre>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference.RunInferenceOnKeyedBatches","title":"RunInferenceOnKeyedBatches","text":"<pre><code>RunInferenceOnKeyedBatches(\n    examples: PCollection,\n    inference_spec_type: InferenceSpecType,\n    load_override_fn: Optional[LoadOverrideFnType] = None,\n) -&gt; PCollection\n</code></pre> <p>Run inference over pre-batched keyed inputs.</p> <p>This API is experimental and may change in the future.</p> <p>Supports the same inference specs as RunInference. Inputs must consist of a keyed list of examples, and outputs consist of keyed list of prediction logs corresponding by index.</p> <p>examples: A PCollection of keyed, batched inputs of type Example,     SequenceExample, or bytes. Each type support inference specs corresponding     to the unbatched cases described in RunInference. Supports       - PCollection[Tuple[K, List[Example]]]       - PCollection[Tuple[K, List[SequenceExample]]]       - PCollection[Tuple[K, List[Bytes]]]   inference_spec_type: Model inference endpoint.   load_override_fn: Optional function taking a model path and sequence of     tags, and returning a tf SavedModel. The loaded model must be equivalent     in interface to the model that would otherwise be loaded. It is up to the     caller to ensure compatibility. This argument is experimental and subject     to change.</p> <p>A PCollection of Tuple[K, List[PredictionLog]].</p> Source code in <code>tfx_bsl/public/beam/run_inference.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(_KeyedBatchesInput)\n@beam.typehints.with_output_types(Tuple[_K, List[_OUTPUT_TYPE]])\ndef RunInferenceOnKeyedBatches(\n    examples: beam.pvalue.PCollection,\n    inference_spec_type: model_spec_pb2.InferenceSpecType,\n    load_override_fn: Optional[run_inference.LoadOverrideFnType] = None,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Run inference over pre-batched keyed inputs.\n\n    This API is experimental and may change in the future.\n\n    Supports the same inference specs as RunInference. Inputs must consist of a\n    keyed list of examples, and outputs consist of keyed list of prediction logs\n    corresponding by index.\n\n    Args:\n    ----\n      examples: A PCollection of keyed, batched inputs of type Example,\n        SequenceExample, or bytes. Each type support inference specs corresponding\n        to the unbatched cases described in RunInference. Supports\n          - PCollection[Tuple[K, List[Example]]]\n          - PCollection[Tuple[K, List[SequenceExample]]]\n          - PCollection[Tuple[K, List[Bytes]]]\n      inference_spec_type: Model inference endpoint.\n      load_override_fn: Optional function taking a model path and sequence of\n        tags, and returning a tf SavedModel. The loaded model must be equivalent\n        in interface to the model that would otherwise be loaded. It is up to the\n        caller to ensure compatibility. This argument is experimental and subject\n        to change.\n\n    Returns:\n    -------\n      A PCollection of Tuple[K, List[PredictionLog]].\n    \"\"\"\n    return (\n        examples\n        | \"RunInferenceOnKeyedBatchesImpl\"\n        &gt;&gt; run_inference.RunInferenceImpl(inference_spec_type, load_override_fn)\n    )\n</code></pre>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference.RunInferencePerModel","title":"RunInferencePerModel","text":"<pre><code>RunInferencePerModel(\n    examples: PCollection,\n    inference_spec_types: Iterable[InferenceSpecType],\n    load_override_fn: Optional[LoadOverrideFnType] = None,\n) -&gt; PCollection\n</code></pre> <p>Vectorized variant of RunInference (useful for ensembles).</p> <p>examples: A PCollection containing examples of the following possible kinds,     each with their corresponding return type.       - PCollection[Example]                  -&gt; PCollection[                                                    Tuple[PredictionLog, ...]]           * Works with Classify, Regress, MultiInference, Predict and             RemotePredict.</p> <pre><code>  - PCollection[SequenceExample]          -&gt; PCollection[\n                                               Tuple[PredictionLog, ...]]\n      * Works with Predict and (serialized) RemotePredict.\n\n  - PCollection[bytes]                    -&gt; PCollection[\n                                               Tuple[PredictionLog, ...]]\n      * For serialized Example: Works with Classify, Regress,\n        MultiInference, Predict and RemotePredict.\n      * For everything else: Works with Predict and RemotePredict.\n\n  - PCollection[Tuple[K, Example]]        -&gt; PCollection[\n                                               Tuple[K,\n                                                     Tuple[PredictionLog,\n                                                           ...]]]\n      * Works with Classify, Regress, MultiInference, Predict and\n        RemotePredict.\n\n  - PCollection[Tuple[K, SequenceExample]] -&gt; PCollection[\n                                               Tuple[K,\n                                                     Tuple[PredictionLog,\n                                                           ...]]]\n      * Works with Predict and (serialized) RemotePredict.\n\n  - PCollection[Tuple[K, bytes]]           -&gt; PCollection[\n                                               Tuple[K,\n                                                     Tuple[PredictionLog,\n                                                           ...]]]\n      * For serialized Example: Works with Classify, Regress,\n        MultiInference, Predict and RemotePredict.\n      * For everything else: Works with Predict and RemotePredict.\n</code></pre> <p>inference_spec_types: A flat iterable of Model inference endpoints.     Inference will happen in a fused fashion (ie without data     materialization), sequentially across Models within a Beam thread (but     in parallel across threads and workers).   load_override_fn: Optional function taking a model path and sequence of     tags, and returning a tf SavedModel. The loaded model must be equivalent     in interface to the model that would otherwise be loaded. It is up to the     caller to ensure compatibility. This argument is experimental and subject     to change.</p> <p>A PCollection (possibly keyed) containing a Tuple of prediction logs. The   Tuple of prediction logs is 1-1 aligned with inference_spec_types.</p> Source code in <code>tfx_bsl/public/beam/run_inference.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(_MaybeKeyedInput)\n@beam.typehints.with_output_types(\n    Union[Tuple[_OUTPUT_TYPE, ...], Tuple[_K, Tuple[_OUTPUT_TYPE, ...]]]\n)\ndef RunInferencePerModel(\n    examples: beam.pvalue.PCollection,\n    inference_spec_types: Iterable[model_spec_pb2.InferenceSpecType],\n    load_override_fn: Optional[run_inference.LoadOverrideFnType] = None,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Vectorized variant of RunInference (useful for ensembles).\n\n    Args:\n    ----\n      examples: A PCollection containing examples of the following possible kinds,\n        each with their corresponding return type.\n          - PCollection[Example]                  -&gt; PCollection[\n                                                       Tuple[PredictionLog, ...]]\n              * Works with Classify, Regress, MultiInference, Predict and\n                RemotePredict.\n\n          - PCollection[SequenceExample]          -&gt; PCollection[\n                                                       Tuple[PredictionLog, ...]]\n              * Works with Predict and (serialized) RemotePredict.\n\n          - PCollection[bytes]                    -&gt; PCollection[\n                                                       Tuple[PredictionLog, ...]]\n              * For serialized Example: Works with Classify, Regress,\n                MultiInference, Predict and RemotePredict.\n              * For everything else: Works with Predict and RemotePredict.\n\n          - PCollection[Tuple[K, Example]]        -&gt; PCollection[\n                                                       Tuple[K,\n                                                             Tuple[PredictionLog,\n                                                                   ...]]]\n              * Works with Classify, Regress, MultiInference, Predict and\n                RemotePredict.\n\n          - PCollection[Tuple[K, SequenceExample]] -&gt; PCollection[\n                                                       Tuple[K,\n                                                             Tuple[PredictionLog,\n                                                                   ...]]]\n              * Works with Predict and (serialized) RemotePredict.\n\n          - PCollection[Tuple[K, bytes]]           -&gt; PCollection[\n                                                       Tuple[K,\n                                                             Tuple[PredictionLog,\n                                                                   ...]]]\n              * For serialized Example: Works with Classify, Regress,\n                MultiInference, Predict and RemotePredict.\n              * For everything else: Works with Predict and RemotePredict.\n\n      inference_spec_types: A flat iterable of Model inference endpoints.\n        Inference will happen in a fused fashion (ie without data\n        materialization), sequentially across Models within a Beam thread (but\n        in parallel across threads and workers).\n      load_override_fn: Optional function taking a model path and sequence of\n        tags, and returning a tf SavedModel. The loaded model must be equivalent\n        in interface to the model that would otherwise be loaded. It is up to the\n        caller to ensure compatibility. This argument is experimental and subject\n        to change.\n\n    Returns:\n    -------\n      A PCollection (possibly keyed) containing a Tuple of prediction logs. The\n      Tuple of prediction logs is 1-1 aligned with inference_spec_types.\n    \"\"\"\n    return (\n        examples\n        | \"RunInferencePerModelImpl\"\n        &gt;&gt; run_inference.RunInferencePerModelImpl(\n            inference_spec_types, load_override_fn\n        )\n    )\n</code></pre>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference.RunInferencePerModelOnKeyedBatches","title":"RunInferencePerModelOnKeyedBatches","text":"<pre><code>RunInferencePerModelOnKeyedBatches(\n    examples: PCollection,\n    inference_spec_types: Iterable[InferenceSpecType],\n    load_override_fn: Optional[LoadOverrideFnType] = None,\n) -&gt; PCollection\n</code></pre> <p>Run inference over pre-batched keyed inputs on multiple models.</p> <p>This API is experimental and may change in the future.</p> <p>Supports the same inference specs as RunInferencePerModel. Inputs must consist of a keyed list of examples, and outputs consist of keyed list of prediction logs corresponding by index.</p> <p>examples: A PCollection of keyed, batched inputs of type Example,     SequenceExample, or bytes. Each type support inference specs corresponding     to the unbatched cases described in RunInferencePerModel. Supports -     PCollection[Tuple[K, List[Example]]] - PCollection[Tuple[K,     List[SequenceExample]]] - PCollection[Tuple[K, List[Bytes]]]   inference_spec_types: A flat iterable of Model inference endpoints.     Inference will happen in a fused fashion (ie without data     materialization), sequentially across Models within a Beam thread (but in     parallel across threads and workers).   load_override_fn: Optional function taking a model path and sequence of     tags, and returning a tf SavedModel. The loaded model must be equivalent     in interface to the model that would otherwise be loaded. It is up to the     caller to ensure compatibility. This argument is experimental and subject     to change.</p> <p>A PCollection containing Tuples of a key and lists of batched prediction   logs from each model provided in inference_spec_types. The Tuple of batched   prediction logs is 1-1 aligned with inference_spec_types. The individual   prediction logs in the batch are 1-1 aligned with the rows of data in the   batch key.</p> Source code in <code>tfx_bsl/public/beam/run_inference.py</code> <pre><code>@beam.ptransform_fn\n@beam.typehints.with_input_types(_KeyedBatchesInput)\n@beam.typehints.with_output_types(Tuple[_K, Tuple[List[_OUTPUT_TYPE]]])\ndef RunInferencePerModelOnKeyedBatches(\n    examples: beam.pvalue.PCollection,\n    inference_spec_types: Iterable[model_spec_pb2.InferenceSpecType],\n    load_override_fn: Optional[run_inference.LoadOverrideFnType] = None,\n) -&gt; beam.pvalue.PCollection:\n    \"\"\"Run inference over pre-batched keyed inputs on multiple models.\n\n    This API is experimental and may change in the future.\n\n    Supports the same inference specs as RunInferencePerModel. Inputs must consist\n    of a keyed list of examples, and outputs consist of keyed list of prediction\n    logs corresponding by index.\n\n    Args:\n    ----\n      examples: A PCollection of keyed, batched inputs of type Example,\n        SequenceExample, or bytes. Each type support inference specs corresponding\n        to the unbatched cases described in RunInferencePerModel. Supports -\n        PCollection[Tuple[K, List[Example]]] - PCollection[Tuple[K,\n        List[SequenceExample]]] - PCollection[Tuple[K, List[Bytes]]]\n      inference_spec_types: A flat iterable of Model inference endpoints.\n        Inference will happen in a fused fashion (ie without data\n        materialization), sequentially across Models within a Beam thread (but in\n        parallel across threads and workers).\n      load_override_fn: Optional function taking a model path and sequence of\n        tags, and returning a tf SavedModel. The loaded model must be equivalent\n        in interface to the model that would otherwise be loaded. It is up to the\n        caller to ensure compatibility. This argument is experimental and subject\n        to change.\n\n    Returns:\n    -------\n      A PCollection containing Tuples of a key and lists of batched prediction\n      logs from each model provided in inference_spec_types. The Tuple of batched\n      prediction logs is 1-1 aligned with inference_spec_types. The individual\n      prediction logs in the batch are 1-1 aligned with the rows of data in the\n      batch key.\n    \"\"\"\n    return (\n        examples\n        | \"RunInferencePerModelOnKeyedBatchesImpl\"\n        &gt;&gt; run_inference.RunInferencePerModelImpl(\n            inference_spec_types, load_override_fn\n        )\n    )\n</code></pre>"},{"location":"api_docs/python/beam/#tfx_bsl.public.beam.run_inference-modules","title":"Modules","text":""},{"location":"api_docs/python/tfxio/","title":"TFX-BSL Public TFXIO","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio","title":"tfx_bsl.public.tfxio","text":"<p>Module level imports for tfx_bsl.public.tfxio.</p> <p>TFXIO defines a common in-memory data representation shared by all TFX libraries and components, as well as an I/O abstraction layer to produce such representations. See the RFC for details: https://github.com/tensorflow/community/blob/master/rfcs/20191017-tfx-standardized-inputs.md</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorRepresentations","title":"TensorRepresentations  <code>module-attribute</code>","text":"<pre><code>TensorRepresentations = Dict[str, TensorRepresentation]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio-classes","title":"Classes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO","title":"BeamRecordCsvTFXIO","text":"<pre><code>BeamRecordCsvTFXIO(\n    physical_format: str,\n    column_names: List[str],\n    delimiter: Optional[str] = \",\",\n    skip_blank_lines: bool = True,\n    multivalent_columns: Optional[str] = None,\n    secondary_delimiter: Optional[str] = None,\n    schema: Optional[Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n    telemetry_descriptors: Optional[List[str]] = None,\n)\n</code></pre> <p>               Bases: <code>_CsvTFXIOBase</code></p> <p>TFXIO implementation for CSV records in pcoll[bytes].</p> <p>This is a special TFXIO that does not actually do I/O -- it relies on the caller to prepare a PCollection of bytes.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def __init__(\n    self,\n    physical_format: str,\n    column_names: List[str],\n    delimiter: Optional[str] = \",\",\n    skip_blank_lines: bool = True,\n    multivalent_columns: Optional[str] = None,\n    secondary_delimiter: Optional[str] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n    telemetry_descriptors: Optional[List[str]] = None,\n):\n    super().__init__(\n        telemetry_descriptors=telemetry_descriptors,\n        raw_record_column_name=raw_record_column_name,\n        logical_format=\"csv\",\n        physical_format=physical_format,\n    )\n    self._schema = schema\n    self._column_names = column_names\n    self._delimiter = delimiter\n    self._skip_blank_lines = skip_blank_lines\n    self._multivalent_columns = multivalent_columns\n    self._secondary_delimiter = secondary_delimiter\n    self._raw_record_column_name = raw_record_column_name\n    if schema is not None:\n        feature_names = [f.name for f in schema.feature]\n        if not set(feature_names).issubset(set(column_names)):\n            raise ValueError(\n                f\"Schema features are not a subset of column names: {column_names} vs {feature_names}\"\n            )\n    self._schema_projected = False\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.raw_record_column_name","title":"raw_record_column_name  <code>property</code>","text":"<pre><code>raw_record_column_name: Optional[str]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.telemetry_descriptors","title":"telemetry_descriptors  <code>property</code>","text":"<pre><code>telemetry_descriptors: Optional[List[str]]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.ArrowSchema","title":"ArrowSchema","text":"<pre><code>ArrowSchema() -&gt; Schema\n</code></pre> <p>Returns the schema of the <code>RecordBatch</code> produced by <code>self.BeamSource()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def ArrowSchema(self) -&gt; pa.Schema:\n    schema = self._ArrowSchemaNoRawRecordColumn()\n    if self._raw_record_column_name is not None:\n        if schema.get_field_index(self._raw_record_column_name) != -1:\n            raise ValueError(\n                f\"Raw record column name {self._raw_record_column_name} collided with a column in the schema.\"\n            )\n        schema = schema.append(\n            pa.field(self._raw_record_column_name, pa.large_list(pa.large_binary()))\n        )\n    return schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.BeamSource","title":"BeamSource","text":"<pre><code>BeamSource(batch_size: Optional[int] = None) -&gt; PTransform\n</code></pre> <p>Returns a beam <code>PTransform</code> that produces <code>PCollection[pa.RecordBatch]</code>.</p> <p>May NOT raise an error if the TFMD schema was not provided at construction time.</p> <p>If a TFMD schema was provided at construction time, all the <code>pa.RecordBatch</code>es in the result <code>PCollection</code> must be of the same schema returned by <code>self.ArrowSchema</code>. If a TFMD schema was not provided, the <code>pa.RecordBatch</code>es might not be of the same schema (they may contain different numbers of columns).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def BeamSource(self, batch_size: Optional[int] = None) -&gt; beam.PTransform:\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        \"\"\"Converts raw records to RecordBatches.\"\"\"\n        return (\n            pcoll_or_pipeline\n            | \"RawRecordBeamSource\" &gt;&gt; self.RawRecordBeamSource()\n            | \"RawRecordToRecordBatch\" &gt;&gt; self.RawRecordToRecordBatch(batch_size)\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.Project","title":"Project","text":"<pre><code>Project(tensor_names: List[str]) -&gt; TFXIO\n</code></pre> <p>Projects the dataset represented by this TFXIO.</p> <p>A Projected TFXIO: - Only columns needed for given tensor_names are guaranteed to be   produced by <code>self.BeamSource()</code> - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed   to contain only those tensors. - It retains a reference to the very original TFXIO, so its TensorAdapter   knows about the specs of the tensors that would be produced by the   original TensorAdapter. Also see <code>TensorAdapter.OriginalTensorSpec()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>tensor_names: a set of tensor names.</p> <p>A <code>TFXIO</code> instance that is the same as <code>self</code> except that:   - Only columns needed for given tensor_names are guaranteed to be     produced by <code>self.BeamSource()</code>   - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed     to contain only those tensors.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def Project(self, tensor_names: List[str]) -&gt; \"TFXIO\":\n    \"\"\"Projects the dataset represented by this TFXIO.\n\n    A Projected TFXIO:\n    - Only columns needed for given tensor_names are guaranteed to be\n      produced by `self.BeamSource()`\n    - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n      to contain only those tensors.\n    - It retains a reference to the very original TFXIO, so its TensorAdapter\n      knows about the specs of the tensors that would be produced by the\n      original TensorAdapter. Also see `TensorAdapter.OriginalTensorSpec()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      tensor_names: a set of tensor names.\n\n    Returns:\n    -------\n      A `TFXIO` instance that is the same as `self` except that:\n      - Only columns needed for given tensor_names are guaranteed to be\n        produced by `self.BeamSource()`\n      - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n        to contain only those tensors.\n    \"\"\"\n    if isinstance(self, _ProjectedTFXIO):\n        # pylint: disable=protected-access\n        return _ProjectedTFXIO(\n            self.origin, self.projected._ProjectImpl(tensor_names)\n        )\n    return _ProjectedTFXIO(self, self._ProjectImpl(tensor_names))\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.RawRecordBeamSource","title":"RawRecordBeamSource","text":"<pre><code>RawRecordBeamSource() -&gt; PTransform\n</code></pre> <p>Returns a PTransform that produces a PCollection[bytes].</p> <p>Used together with RawRecordToRecordBatch(), it allows getting both the PCollection of the raw records and the PCollection of the RecordBatch from the same source. For example:</p> <p>record_batch = pipeline | tfxio.BeamSource() raw_record = pipeline | tfxio.RawRecordBeamSource()</p> <p>would result in the files being read twice, while the following would only read once:</p> <p>raw_record = pipeline | tfxio.RawRecordBeamSource() record_batch = raw_record | tfxio.RawRecordToRecordBatch()</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordBeamSource(self) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that produces a PCollection[bytes].\n\n    Used together with RawRecordToRecordBatch(), it allows getting both the\n    PCollection of the raw records and the PCollection of the RecordBatch from\n    the same source. For example:\n\n    record_batch = pipeline | tfxio.BeamSource()\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n\n    would result in the files being read twice, while the following would only\n    read once:\n\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n    record_batch = raw_record | tfxio.RawRecordToRecordBatch()\n    \"\"\"\n\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(bytes)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        return (\n            pcoll_or_pipeline\n            | \"ReadRawRecords\" &gt;&gt; self._RawRecordBeamSourceInternal()\n            | \"CollectRawRecordTelemetry\"\n            &gt;&gt; telemetry.ProfileRawRecords(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.RawRecordTensorFlowDataset","title":"RawRecordTensorFlowDataset","text":"<pre><code>RawRecordTensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Returns a Dataset that contains nested Datasets of raw records.</p> <p>May not be implemented for some TFXIOs.</p> <p>This should be used when RawTfRecordTFXIO.TensorFlowDataset does not suffice. Namely, if there is some logical grouping of files which we need to perform operations on, without applying the operation to each individual group (i.e. shuffle).</p> <p>The returned Dataset object is a dataset of datasets, where each nested dataset is a dataset of serialized records. When shuffle=False (default), the nested datasets are deterministically ordered. Each nested dataset can represent multiple files. The files are merged into one dataset if the files have the same format. For example:</p> <p><pre><code>file_patterns = ['file_1', 'file_2', 'dir_1/*']\nfile_formats = ['recordio', 'recordio', 'sstable']\ntfxio = SomeTFXIO(file_patterns, file_formats)\ndatasets = tfxio.RawRecordTensorFlowDataset(options)\n</code></pre> <code>datasets</code> would result in the following dataset: <code>[ds1, ds2]</code>. Where ds1 iterates over records from 'file_1' and 'file_2', and ds2 iterates over records from files matched by 'dir_1/*'.</p> <p>Example usage: <pre><code>tfxio = SomeTFXIO(file_patterns, file_formats)\nds = tfxio.RawRecordTensorFlowDataset(options=options)\nds = ds.flat_map(lambda x: x)\nrecords = list(ds.as_numpy_iterator())\n# iterating over `records` yields records from the each file in\n# `file_patterns`. See `tf.data.Dataset.list_files` for more information\n# about the order of files when expanding globs.\n</code></pre> Note that we need a flat_map, because <code>RawRecordTensorFlowDataset</code> returns a dataset of datasets.</p> <p>When shuffle=True, then the datasets not deterministically ordered, but the contents of each nested dataset are deterministcally ordered. For example, we may potentially have [ds2, ds1, ds3], where the contents of ds1, ds2, and ds3 are all deterministcally ordered.</p> <p>options: A TensorFlowDatasetOptions object. Not all options will apply.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordTensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Returns a Dataset that contains nested Datasets of raw records.\n\n    May not be implemented for some TFXIOs.\n\n    This should be used when RawTfRecordTFXIO.TensorFlowDataset does not\n    suffice. Namely, if there is some logical grouping of files which we need\n    to perform operations on, without applying the operation to each individual\n    group (i.e. shuffle).\n\n    The returned Dataset object is a dataset of datasets, where each nested\n    dataset is a dataset of serialized records. When shuffle=False (default),\n    the nested datasets are deterministically ordered. Each nested dataset can\n    represent multiple files. The files are merged into one dataset if the files\n    have the same format. For example:\n\n    ```\n    file_patterns = ['file_1', 'file_2', 'dir_1/*']\n    file_formats = ['recordio', 'recordio', 'sstable']\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    datasets = tfxio.RawRecordTensorFlowDataset(options)\n    ```\n    `datasets` would result in the following dataset: `[ds1, ds2]`. Where ds1\n    iterates over records from 'file_1' and 'file_2', and ds2 iterates over\n    records from files matched by 'dir_1/*'.\n\n    Example usage:\n    ```\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    ds = tfxio.RawRecordTensorFlowDataset(options=options)\n    ds = ds.flat_map(lambda x: x)\n    records = list(ds.as_numpy_iterator())\n    # iterating over `records` yields records from the each file in\n    # `file_patterns`. See `tf.data.Dataset.list_files` for more information\n    # about the order of files when expanding globs.\n    ```\n    Note that we need a flat_map, because `RawRecordTensorFlowDataset` returns\n    a dataset of datasets.\n\n    When shuffle=True, then the datasets not deterministically ordered,\n    but the contents of each nested dataset are deterministcally ordered.\n    For example, we may potentially have [ds2, ds1, ds3], where the\n    contents of ds1, ds2, and ds3 are all deterministcally ordered.\n\n    Args:\n    ----\n      options: A TensorFlowDatasetOptions object. Not all options will apply.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.RawRecordToRecordBatch","title":"RawRecordToRecordBatch","text":"<pre><code>RawRecordToRecordBatch(\n    batch_size: Optional[int] = None,\n) -&gt; PTransform\n</code></pre> <p>Returns a PTransform that converts raw records to Arrow RecordBatches.</p> <p>The input PCollection must be from self.RawRecordBeamSource() (also see the documentation for that method).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordToRecordBatch(\n    self, batch_size: Optional[int] = None\n) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that converts raw records to Arrow RecordBatches.\n\n    The input PCollection must be from self.RawRecordBeamSource() (also see\n    the documentation for that method).\n\n    Args:\n    ----\n      batch_size: if not None, the `pa.RecordBatch` produced will be of the\n        specified size. Otherwise it's automatically tuned by Beam.\n    \"\"\"\n\n    @beam.typehints.with_input_types(bytes)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll: beam.pvalue.PCollection):\n        return (\n            pcoll\n            | \"RawRecordToRecordBatch\"\n            &gt;&gt; self._RawRecordToRecordBatchInternal(batch_size)\n            | \"CollectRecordBatchTelemetry\"\n            &gt;&gt; telemetry.ProfileRecordBatches(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.RecordBatches","title":"RecordBatches","text":"<pre><code>RecordBatches(options: RecordBatchesOptions)\n</code></pre> <p>Returns an iterable of record batches.</p> <p>This can be used outside of Apache Beam or TensorFlow to access data.</p> <p>options: An options object for iterating over record batches. Look at     <code>dataset_options.RecordBatchesOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def RecordBatches(self, options: dataset_options.RecordBatchesOptions):\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.SupportAttachingRawRecords","title":"SupportAttachingRawRecords","text":"<pre><code>SupportAttachingRawRecords() -&gt; bool\n</code></pre> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def SupportAttachingRawRecords(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter() -&gt; TensorAdapter\n</code></pre> <p>Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapter(self) -&gt; tensor_adapter.TensorAdapter:\n    \"\"\"Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n    return tensor_adapter.TensorAdapter(self.TensorAdapterConfig())\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig() -&gt; TensorAdapterConfig\n</code></pre> <p>Returns the config to initialize a <code>TensorAdapter</code>.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.TensorAdapterConfig--returns","title":"Returns","text":"<p>a <code>TensorAdapterConfig</code> that is the same as what is used to initialize the   <code>TensorAdapter</code> returned by <code>self.TensorAdapter()</code>.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapterConfig(self) -&gt; tensor_adapter.TensorAdapterConfig:\n    \"\"\"Returns the config to initialize a `TensorAdapter`.\n\n    Returns\n    -------\n      a `TensorAdapterConfig` that is the same as what is used to initialize the\n      `TensorAdapter` returned by `self.TensorAdapter()`.\n    \"\"\"\n    return tensor_adapter.TensorAdapterConfig(\n        self.ArrowSchema(), self.TensorRepresentations()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.TensorFlowDataset","title":"TensorFlowDataset","text":"<pre><code>TensorFlowDataset(options: TensorFlowDatasetOptions)\n</code></pre> <p>Returns a tf.data.Dataset of TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>options: an options object for the tf.data.Dataset. Look at     <code>dataset_options.TensorFlowDatasetOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def TensorFlowDataset(self, options: dataset_options.TensorFlowDatasetOptions):\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.BeamRecordCsvTFXIO.TensorRepresentations","title":"TensorRepresentations","text":"<pre><code>TensorRepresentations() -&gt; TensorRepresentations\n</code></pre> <p>Returns the <code>TensorRepresentations</code>.</p> <p>These <code>TensorRepresentation</code>s describe the tensors or composite tensors produced by the <code>TensorAdapter</code> created from <code>self.TensorAdapter()</code> or the tf.data.Dataset created from <code>self.TensorFlowDataset()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time. May raise an error if the tensor representations are invalid.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def TensorRepresentations(self) -&gt; tensor_adapter.TensorRepresentations:\n    return self._TensorRepresentations(not self._schema_projected)\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO","title":"CsvTFXIO","text":"<pre><code>CsvTFXIO(\n    file_pattern: str,\n    column_names: List[str],\n    telemetry_descriptors: Optional[List[str]] = None,\n    validate: bool = True,\n    delimiter: Optional[str] = \",\",\n    skip_blank_lines: Optional[bool] = True,\n    multivalent_columns: Optional[str] = None,\n    secondary_delimiter: Optional[str] = None,\n    schema: Optional[Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n    skip_header_lines: int = 0,\n)\n</code></pre> <p>               Bases: <code>_CsvTFXIOBase</code></p> <p>TFXIO implementation for CSV.</p> <p>Initializes a CSV TFXIO.</p> <p>file_pattern: A file glob pattern to read csv files from.   column_names: List of csv column names. Order must match the order in the     CSV file.   telemetry_descriptors: A set of descriptors that identify the component     that is instantiating this TFXIO. These will be used to construct the     namespace to contain metrics for profiling and are therefore expected to     be identifiers of the component itself and not individual instances of     source use.   validate: Boolean flag to verify that the files exist during the pipeline     creation time.   delimiter: A one-character string used to separate fields.   skip_blank_lines: A boolean to indicate whether to skip over blank lines     rather than interpreting them as missing values.   multivalent_columns: Name of column that can contain multiple values. If     secondary_delimiter is provided, this must also be provided.   secondary_delimiter: Delimiter used for parsing multivalent columns. If     multivalent_columns is provided, this must also be provided.   schema: An optional TFMD Schema describing the dataset. If schema is     provided, it will determine the data type of the csv columns. Otherwise,     the each column's data type will be inferred by the csv decoder. The     schema should contain exactly the same features as column_names.   raw_record_column_name: If not None, the generated Arrow RecordBatches     will contain a column of the given name that contains raw csv rows.   skip_header_lines: Number of header lines to skip. Same number is     skipped from each file. Must be 0 or higher. Large number of     skipped lines might impact performance.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def __init__(\n    self,\n    file_pattern: str,\n    column_names: List[str],\n    telemetry_descriptors: Optional[List[str]] = None,\n    validate: bool = True,\n    delimiter: Optional[str] = \",\",\n    skip_blank_lines: Optional[bool] = True,\n    multivalent_columns: Optional[str] = None,\n    secondary_delimiter: Optional[str] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n    skip_header_lines: int = 0,\n):\n    \"\"\"Initializes a CSV TFXIO.\n\n    Args:\n    ----\n      file_pattern: A file glob pattern to read csv files from.\n      column_names: List of csv column names. Order must match the order in the\n        CSV file.\n      telemetry_descriptors: A set of descriptors that identify the component\n        that is instantiating this TFXIO. These will be used to construct the\n        namespace to contain metrics for profiling and are therefore expected to\n        be identifiers of the component itself and not individual instances of\n        source use.\n      validate: Boolean flag to verify that the files exist during the pipeline\n        creation time.\n      delimiter: A one-character string used to separate fields.\n      skip_blank_lines: A boolean to indicate whether to skip over blank lines\n        rather than interpreting them as missing values.\n      multivalent_columns: Name of column that can contain multiple values. If\n        secondary_delimiter is provided, this must also be provided.\n      secondary_delimiter: Delimiter used for parsing multivalent columns. If\n        multivalent_columns is provided, this must also be provided.\n      schema: An optional TFMD Schema describing the dataset. If schema is\n        provided, it will determine the data type of the csv columns. Otherwise,\n        the each column's data type will be inferred by the csv decoder. The\n        schema should contain exactly the same features as column_names.\n      raw_record_column_name: If not None, the generated Arrow RecordBatches\n        will contain a column of the given name that contains raw csv rows.\n      skip_header_lines: Number of header lines to skip. Same number is\n        skipped from each file. Must be 0 or higher. Large number of\n        skipped lines might impact performance.\n    \"\"\"\n    super().__init__(\n        column_names=column_names,\n        delimiter=delimiter,\n        skip_blank_lines=skip_blank_lines,\n        multivalent_columns=multivalent_columns,\n        secondary_delimiter=secondary_delimiter,\n        schema=schema,\n        raw_record_column_name=raw_record_column_name,\n        telemetry_descriptors=telemetry_descriptors,\n        physical_format=\"text\",\n    )\n    self._file_pattern = file_pattern\n    self._validate = validate\n    self._skip_header_lines = skip_header_lines\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.raw_record_column_name","title":"raw_record_column_name  <code>property</code>","text":"<pre><code>raw_record_column_name: Optional[str]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.telemetry_descriptors","title":"telemetry_descriptors  <code>property</code>","text":"<pre><code>telemetry_descriptors: Optional[List[str]]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.ArrowSchema","title":"ArrowSchema","text":"<pre><code>ArrowSchema() -&gt; Schema\n</code></pre> <p>Returns the schema of the <code>RecordBatch</code> produced by <code>self.BeamSource()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def ArrowSchema(self) -&gt; pa.Schema:\n    schema = self._ArrowSchemaNoRawRecordColumn()\n    if self._raw_record_column_name is not None:\n        if schema.get_field_index(self._raw_record_column_name) != -1:\n            raise ValueError(\n                f\"Raw record column name {self._raw_record_column_name} collided with a column in the schema.\"\n            )\n        schema = schema.append(\n            pa.field(self._raw_record_column_name, pa.large_list(pa.large_binary()))\n        )\n    return schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.BeamSource","title":"BeamSource","text":"<pre><code>BeamSource(batch_size: Optional[int] = None) -&gt; PTransform\n</code></pre> <p>Returns a beam <code>PTransform</code> that produces <code>PCollection[pa.RecordBatch]</code>.</p> <p>May NOT raise an error if the TFMD schema was not provided at construction time.</p> <p>If a TFMD schema was provided at construction time, all the <code>pa.RecordBatch</code>es in the result <code>PCollection</code> must be of the same schema returned by <code>self.ArrowSchema</code>. If a TFMD schema was not provided, the <code>pa.RecordBatch</code>es might not be of the same schema (they may contain different numbers of columns).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def BeamSource(self, batch_size: Optional[int] = None) -&gt; beam.PTransform:\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        \"\"\"Converts raw records to RecordBatches.\"\"\"\n        return (\n            pcoll_or_pipeline\n            | \"RawRecordBeamSource\" &gt;&gt; self.RawRecordBeamSource()\n            | \"RawRecordToRecordBatch\" &gt;&gt; self.RawRecordToRecordBatch(batch_size)\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.Project","title":"Project","text":"<pre><code>Project(tensor_names: List[str]) -&gt; TFXIO\n</code></pre> <p>Projects the dataset represented by this TFXIO.</p> <p>A Projected TFXIO: - Only columns needed for given tensor_names are guaranteed to be   produced by <code>self.BeamSource()</code> - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed   to contain only those tensors. - It retains a reference to the very original TFXIO, so its TensorAdapter   knows about the specs of the tensors that would be produced by the   original TensorAdapter. Also see <code>TensorAdapter.OriginalTensorSpec()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>tensor_names: a set of tensor names.</p> <p>A <code>TFXIO</code> instance that is the same as <code>self</code> except that:   - Only columns needed for given tensor_names are guaranteed to be     produced by <code>self.BeamSource()</code>   - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed     to contain only those tensors.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def Project(self, tensor_names: List[str]) -&gt; \"TFXIO\":\n    \"\"\"Projects the dataset represented by this TFXIO.\n\n    A Projected TFXIO:\n    - Only columns needed for given tensor_names are guaranteed to be\n      produced by `self.BeamSource()`\n    - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n      to contain only those tensors.\n    - It retains a reference to the very original TFXIO, so its TensorAdapter\n      knows about the specs of the tensors that would be produced by the\n      original TensorAdapter. Also see `TensorAdapter.OriginalTensorSpec()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      tensor_names: a set of tensor names.\n\n    Returns:\n    -------\n      A `TFXIO` instance that is the same as `self` except that:\n      - Only columns needed for given tensor_names are guaranteed to be\n        produced by `self.BeamSource()`\n      - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n        to contain only those tensors.\n    \"\"\"\n    if isinstance(self, _ProjectedTFXIO):\n        # pylint: disable=protected-access\n        return _ProjectedTFXIO(\n            self.origin, self.projected._ProjectImpl(tensor_names)\n        )\n    return _ProjectedTFXIO(self, self._ProjectImpl(tensor_names))\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.RawRecordBeamSource","title":"RawRecordBeamSource","text":"<pre><code>RawRecordBeamSource() -&gt; PTransform\n</code></pre> <p>Returns a PTransform that produces a PCollection[bytes].</p> <p>Used together with RawRecordToRecordBatch(), it allows getting both the PCollection of the raw records and the PCollection of the RecordBatch from the same source. For example:</p> <p>record_batch = pipeline | tfxio.BeamSource() raw_record = pipeline | tfxio.RawRecordBeamSource()</p> <p>would result in the files being read twice, while the following would only read once:</p> <p>raw_record = pipeline | tfxio.RawRecordBeamSource() record_batch = raw_record | tfxio.RawRecordToRecordBatch()</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordBeamSource(self) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that produces a PCollection[bytes].\n\n    Used together with RawRecordToRecordBatch(), it allows getting both the\n    PCollection of the raw records and the PCollection of the RecordBatch from\n    the same source. For example:\n\n    record_batch = pipeline | tfxio.BeamSource()\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n\n    would result in the files being read twice, while the following would only\n    read once:\n\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n    record_batch = raw_record | tfxio.RawRecordToRecordBatch()\n    \"\"\"\n\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(bytes)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        return (\n            pcoll_or_pipeline\n            | \"ReadRawRecords\" &gt;&gt; self._RawRecordBeamSourceInternal()\n            | \"CollectRawRecordTelemetry\"\n            &gt;&gt; telemetry.ProfileRawRecords(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.RawRecordTensorFlowDataset","title":"RawRecordTensorFlowDataset","text":"<pre><code>RawRecordTensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Returns a Dataset that contains nested Datasets of raw records.</p> <p>May not be implemented for some TFXIOs.</p> <p>This should be used when RawTfRecordTFXIO.TensorFlowDataset does not suffice. Namely, if there is some logical grouping of files which we need to perform operations on, without applying the operation to each individual group (i.e. shuffle).</p> <p>The returned Dataset object is a dataset of datasets, where each nested dataset is a dataset of serialized records. When shuffle=False (default), the nested datasets are deterministically ordered. Each nested dataset can represent multiple files. The files are merged into one dataset if the files have the same format. For example:</p> <p><pre><code>file_patterns = ['file_1', 'file_2', 'dir_1/*']\nfile_formats = ['recordio', 'recordio', 'sstable']\ntfxio = SomeTFXIO(file_patterns, file_formats)\ndatasets = tfxio.RawRecordTensorFlowDataset(options)\n</code></pre> <code>datasets</code> would result in the following dataset: <code>[ds1, ds2]</code>. Where ds1 iterates over records from 'file_1' and 'file_2', and ds2 iterates over records from files matched by 'dir_1/*'.</p> <p>Example usage: <pre><code>tfxio = SomeTFXIO(file_patterns, file_formats)\nds = tfxio.RawRecordTensorFlowDataset(options=options)\nds = ds.flat_map(lambda x: x)\nrecords = list(ds.as_numpy_iterator())\n# iterating over `records` yields records from the each file in\n# `file_patterns`. See `tf.data.Dataset.list_files` for more information\n# about the order of files when expanding globs.\n</code></pre> Note that we need a flat_map, because <code>RawRecordTensorFlowDataset</code> returns a dataset of datasets.</p> <p>When shuffle=True, then the datasets not deterministically ordered, but the contents of each nested dataset are deterministcally ordered. For example, we may potentially have [ds2, ds1, ds3], where the contents of ds1, ds2, and ds3 are all deterministcally ordered.</p> <p>options: A TensorFlowDatasetOptions object. Not all options will apply.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordTensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Returns a Dataset that contains nested Datasets of raw records.\n\n    May not be implemented for some TFXIOs.\n\n    This should be used when RawTfRecordTFXIO.TensorFlowDataset does not\n    suffice. Namely, if there is some logical grouping of files which we need\n    to perform operations on, without applying the operation to each individual\n    group (i.e. shuffle).\n\n    The returned Dataset object is a dataset of datasets, where each nested\n    dataset is a dataset of serialized records. When shuffle=False (default),\n    the nested datasets are deterministically ordered. Each nested dataset can\n    represent multiple files. The files are merged into one dataset if the files\n    have the same format. For example:\n\n    ```\n    file_patterns = ['file_1', 'file_2', 'dir_1/*']\n    file_formats = ['recordio', 'recordio', 'sstable']\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    datasets = tfxio.RawRecordTensorFlowDataset(options)\n    ```\n    `datasets` would result in the following dataset: `[ds1, ds2]`. Where ds1\n    iterates over records from 'file_1' and 'file_2', and ds2 iterates over\n    records from files matched by 'dir_1/*'.\n\n    Example usage:\n    ```\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    ds = tfxio.RawRecordTensorFlowDataset(options=options)\n    ds = ds.flat_map(lambda x: x)\n    records = list(ds.as_numpy_iterator())\n    # iterating over `records` yields records from the each file in\n    # `file_patterns`. See `tf.data.Dataset.list_files` for more information\n    # about the order of files when expanding globs.\n    ```\n    Note that we need a flat_map, because `RawRecordTensorFlowDataset` returns\n    a dataset of datasets.\n\n    When shuffle=True, then the datasets not deterministically ordered,\n    but the contents of each nested dataset are deterministcally ordered.\n    For example, we may potentially have [ds2, ds1, ds3], where the\n    contents of ds1, ds2, and ds3 are all deterministcally ordered.\n\n    Args:\n    ----\n      options: A TensorFlowDatasetOptions object. Not all options will apply.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.RawRecordToRecordBatch","title":"RawRecordToRecordBatch","text":"<pre><code>RawRecordToRecordBatch(\n    batch_size: Optional[int] = None,\n) -&gt; PTransform\n</code></pre> <p>Returns a PTransform that converts raw records to Arrow RecordBatches.</p> <p>The input PCollection must be from self.RawRecordBeamSource() (also see the documentation for that method).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordToRecordBatch(\n    self, batch_size: Optional[int] = None\n) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that converts raw records to Arrow RecordBatches.\n\n    The input PCollection must be from self.RawRecordBeamSource() (also see\n    the documentation for that method).\n\n    Args:\n    ----\n      batch_size: if not None, the `pa.RecordBatch` produced will be of the\n        specified size. Otherwise it's automatically tuned by Beam.\n    \"\"\"\n\n    @beam.typehints.with_input_types(bytes)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll: beam.pvalue.PCollection):\n        return (\n            pcoll\n            | \"RawRecordToRecordBatch\"\n            &gt;&gt; self._RawRecordToRecordBatchInternal(batch_size)\n            | \"CollectRecordBatchTelemetry\"\n            &gt;&gt; telemetry.ProfileRecordBatches(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.RecordBatches","title":"RecordBatches","text":"<pre><code>RecordBatches(options: RecordBatchesOptions)\n</code></pre> <p>Returns an iterable of record batches.</p> <p>This can be used outside of Apache Beam or TensorFlow to access data.</p> <p>options: An options object for iterating over record batches. Look at     <code>dataset_options.RecordBatchesOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def RecordBatches(self, options: dataset_options.RecordBatchesOptions):\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.SupportAttachingRawRecords","title":"SupportAttachingRawRecords","text":"<pre><code>SupportAttachingRawRecords() -&gt; bool\n</code></pre> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def SupportAttachingRawRecords(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter() -&gt; TensorAdapter\n</code></pre> <p>Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapter(self) -&gt; tensor_adapter.TensorAdapter:\n    \"\"\"Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n    return tensor_adapter.TensorAdapter(self.TensorAdapterConfig())\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig() -&gt; TensorAdapterConfig\n</code></pre> <p>Returns the config to initialize a <code>TensorAdapter</code>.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.TensorAdapterConfig--returns","title":"Returns","text":"<p>a <code>TensorAdapterConfig</code> that is the same as what is used to initialize the   <code>TensorAdapter</code> returned by <code>self.TensorAdapter()</code>.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapterConfig(self) -&gt; tensor_adapter.TensorAdapterConfig:\n    \"\"\"Returns the config to initialize a `TensorAdapter`.\n\n    Returns\n    -------\n      a `TensorAdapterConfig` that is the same as what is used to initialize the\n      `TensorAdapter` returned by `self.TensorAdapter()`.\n    \"\"\"\n    return tensor_adapter.TensorAdapterConfig(\n        self.ArrowSchema(), self.TensorRepresentations()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.TensorFlowDataset","title":"TensorFlowDataset","text":"<pre><code>TensorFlowDataset(options: TensorFlowDatasetOptions)\n</code></pre> <p>Returns a tf.data.Dataset of TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>options: an options object for the tf.data.Dataset. Look at     <code>dataset_options.TensorFlowDatasetOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def TensorFlowDataset(self, options: dataset_options.TensorFlowDatasetOptions):\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.CsvTFXIO.TensorRepresentations","title":"TensorRepresentations","text":"<pre><code>TensorRepresentations() -&gt; TensorRepresentations\n</code></pre> <p>Returns the <code>TensorRepresentations</code>.</p> <p>These <code>TensorRepresentation</code>s describe the tensors or composite tensors produced by the <code>TensorAdapter</code> created from <code>self.TensorAdapter()</code> or the tf.data.Dataset created from <code>self.TensorFlowDataset()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time. May raise an error if the tensor representations are invalid.</p> Source code in <code>tfx_bsl/tfxio/csv_tfxio.py</code> <pre><code>def TensorRepresentations(self) -&gt; tensor_adapter.TensorRepresentations:\n    return self._TensorRepresentations(not self._schema_projected)\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.RecordBatchToExamplesEncoder","title":"RecordBatchToExamplesEncoder","text":"<pre><code>RecordBatchToExamplesEncoder(\n    schema: Optional[Schema] = None,\n)\n</code></pre> <p>Encodes <code>pa.RecordBatch</code> as a list of serialized <code>tf.Example</code>s.</p> <p>Requires TFMD schema only if RecordBatches contains nested lists with depth &gt; 2 that represent TensorFlow's RaggedFeatures.</p> Source code in <code>tfx_bsl/coders/example_coder.py</code> <pre><code>def __init__(self, schema: Optional[schema_pb2.Schema] = None):\n    self._schema = schema\n    self._coder = RecordBatchToExamplesEncoderCpp(\n        None if schema is None else schema.SerializeToString()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.RecordBatchToExamplesEncoder-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.RecordBatchToExamplesEncoder.encode","title":"encode","text":"<pre><code>encode(record_batch: RecordBatch) -&gt; List[bytes]\n</code></pre> Source code in <code>tfx_bsl/coders/example_coder.py</code> <pre><code>def encode(self, record_batch: pa.RecordBatch) -&gt; List[bytes]:  # pylint: disable=invalid-name\n    return self._coder.Encode(record_batch)\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.RecordBatchesOptions","title":"RecordBatchesOptions","text":"<p>               Bases: <code>NamedTuple('RecordBatchesOptions', [('batch_size', int), ('drop_final_batch', bool), ('num_epochs', Optional[int]), ('shuffle', bool), ('shuffle_buffer_size', int), ('shuffle_seed', Optional[int])])</code></p> <p>Options for TFXIO's RecordBatches.</p> <p>Note: not all of these options may be effective. It depends on the particular TFXIO's implementation.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord","title":"TFExampleBeamRecord","text":"<pre><code>TFExampleBeamRecord(\n    physical_format: str,\n    telemetry_descriptors: Optional[List[str]] = None,\n    schema: Optional[Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>_TFExampleRecordBase</code></p> <p>TFXIO implementation for serialized tf.Examples in pcoll[bytes].</p> <p>This is a special TFXIO that does not actually do I/O -- it relies on the caller to prepare a PCollection of bytes (serialized tf.Examples).</p> <p>Initializer.</p> <p>physical_format: The physical format that describes where the input     pcoll[bytes] comes from. Used for telemetry purposes. Examples: \"text\",     \"tfrecord\".   telemetry_descriptors: A set of descriptors that identify the component     that is instantiating this TFXIO. These will be used to construct the     namespace to contain metrics for profiling and are therefore expected to     be identifiers of the component itself and not individual instances of     source use.   schema: A TFMD Schema describing the dataset.   raw_record_column_name: If not None, the generated Arrow RecordBatches     will contain a column of the given name that contains serialized     records.</p> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def __init__(\n    self,\n    physical_format: str,\n    telemetry_descriptors: Optional[List[str]] = None,\n    schema: Optional[schema_pb2.Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n):\n    \"\"\"Initializer.\n\n    Args:\n    ----\n      physical_format: The physical format that describes where the input\n        pcoll[bytes] comes from. Used for telemetry purposes. Examples: \"text\",\n        \"tfrecord\".\n      telemetry_descriptors: A set of descriptors that identify the component\n        that is instantiating this TFXIO. These will be used to construct the\n        namespace to contain metrics for profiling and are therefore expected to\n        be identifiers of the component itself and not individual instances of\n        source use.\n      schema: A TFMD Schema describing the dataset.\n      raw_record_column_name: If not None, the generated Arrow RecordBatches\n        will contain a column of the given name that contains serialized\n        records.\n    \"\"\"\n    super().__init__(\n        schema=schema,\n        raw_record_column_name=raw_record_column_name,\n        telemetry_descriptors=telemetry_descriptors,\n        physical_format=physical_format,\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.raw_record_column_name","title":"raw_record_column_name  <code>property</code>","text":"<pre><code>raw_record_column_name: Optional[str]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.telemetry_descriptors","title":"telemetry_descriptors  <code>property</code>","text":"<pre><code>telemetry_descriptors: Optional[List[str]]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.ArrowSchema","title":"ArrowSchema","text":"<pre><code>ArrowSchema() -&gt; Schema\n</code></pre> <p>Returns the schema of the <code>RecordBatch</code> produced by <code>self.BeamSource()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def ArrowSchema(self) -&gt; pa.Schema:\n    schema = self._ArrowSchemaNoRawRecordColumn()\n    if self._raw_record_column_name is not None:\n        if schema.get_field_index(self._raw_record_column_name) != -1:\n            raise ValueError(\n                f\"Raw record column name {self._raw_record_column_name} collided with a column in the schema.\"\n            )\n        schema = schema.append(\n            pa.field(self._raw_record_column_name, pa.large_list(pa.large_binary()))\n        )\n    return schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.BeamSource","title":"BeamSource","text":"<pre><code>BeamSource(batch_size: Optional[int] = None) -&gt; PTransform\n</code></pre> <p>Returns a beam <code>PTransform</code> that produces <code>PCollection[pa.RecordBatch]</code>.</p> <p>May NOT raise an error if the TFMD schema was not provided at construction time.</p> <p>If a TFMD schema was provided at construction time, all the <code>pa.RecordBatch</code>es in the result <code>PCollection</code> must be of the same schema returned by <code>self.ArrowSchema</code>. If a TFMD schema was not provided, the <code>pa.RecordBatch</code>es might not be of the same schema (they may contain different numbers of columns).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def BeamSource(self, batch_size: Optional[int] = None) -&gt; beam.PTransform:\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        \"\"\"Converts raw records to RecordBatches.\"\"\"\n        return (\n            pcoll_or_pipeline\n            | \"RawRecordBeamSource\" &gt;&gt; self.RawRecordBeamSource()\n            | \"RawRecordToRecordBatch\" &gt;&gt; self.RawRecordToRecordBatch(batch_size)\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.Project","title":"Project","text":"<pre><code>Project(tensor_names: List[str]) -&gt; TFXIO\n</code></pre> <p>Projects the dataset represented by this TFXIO.</p> <p>A Projected TFXIO: - Only columns needed for given tensor_names are guaranteed to be   produced by <code>self.BeamSource()</code> - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed   to contain only those tensors. - It retains a reference to the very original TFXIO, so its TensorAdapter   knows about the specs of the tensors that would be produced by the   original TensorAdapter. Also see <code>TensorAdapter.OriginalTensorSpec()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>tensor_names: a set of tensor names.</p> <p>A <code>TFXIO</code> instance that is the same as <code>self</code> except that:   - Only columns needed for given tensor_names are guaranteed to be     produced by <code>self.BeamSource()</code>   - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed     to contain only those tensors.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def Project(self, tensor_names: List[str]) -&gt; \"TFXIO\":\n    \"\"\"Projects the dataset represented by this TFXIO.\n\n    A Projected TFXIO:\n    - Only columns needed for given tensor_names are guaranteed to be\n      produced by `self.BeamSource()`\n    - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n      to contain only those tensors.\n    - It retains a reference to the very original TFXIO, so its TensorAdapter\n      knows about the specs of the tensors that would be produced by the\n      original TensorAdapter. Also see `TensorAdapter.OriginalTensorSpec()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      tensor_names: a set of tensor names.\n\n    Returns:\n    -------\n      A `TFXIO` instance that is the same as `self` except that:\n      - Only columns needed for given tensor_names are guaranteed to be\n        produced by `self.BeamSource()`\n      - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n        to contain only those tensors.\n    \"\"\"\n    if isinstance(self, _ProjectedTFXIO):\n        # pylint: disable=protected-access\n        return _ProjectedTFXIO(\n            self.origin, self.projected._ProjectImpl(tensor_names)\n        )\n    return _ProjectedTFXIO(self, self._ProjectImpl(tensor_names))\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.RawRecordBeamSource","title":"RawRecordBeamSource","text":"<pre><code>RawRecordBeamSource() -&gt; PTransform\n</code></pre> <p>Returns a PTransform that produces a PCollection[bytes].</p> <p>Used together with RawRecordToRecordBatch(), it allows getting both the PCollection of the raw records and the PCollection of the RecordBatch from the same source. For example:</p> <p>record_batch = pipeline | tfxio.BeamSource() raw_record = pipeline | tfxio.RawRecordBeamSource()</p> <p>would result in the files being read twice, while the following would only read once:</p> <p>raw_record = pipeline | tfxio.RawRecordBeamSource() record_batch = raw_record | tfxio.RawRecordToRecordBatch()</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordBeamSource(self) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that produces a PCollection[bytes].\n\n    Used together with RawRecordToRecordBatch(), it allows getting both the\n    PCollection of the raw records and the PCollection of the RecordBatch from\n    the same source. For example:\n\n    record_batch = pipeline | tfxio.BeamSource()\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n\n    would result in the files being read twice, while the following would only\n    read once:\n\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n    record_batch = raw_record | tfxio.RawRecordToRecordBatch()\n    \"\"\"\n\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(bytes)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        return (\n            pcoll_or_pipeline\n            | \"ReadRawRecords\" &gt;&gt; self._RawRecordBeamSourceInternal()\n            | \"CollectRawRecordTelemetry\"\n            &gt;&gt; telemetry.ProfileRawRecords(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.RawRecordTensorFlowDataset","title":"RawRecordTensorFlowDataset","text":"<pre><code>RawRecordTensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Returns a Dataset that contains nested Datasets of raw records.</p> <p>May not be implemented for some TFXIOs.</p> <p>This should be used when RawTfRecordTFXIO.TensorFlowDataset does not suffice. Namely, if there is some logical grouping of files which we need to perform operations on, without applying the operation to each individual group (i.e. shuffle).</p> <p>The returned Dataset object is a dataset of datasets, where each nested dataset is a dataset of serialized records. When shuffle=False (default), the nested datasets are deterministically ordered. Each nested dataset can represent multiple files. The files are merged into one dataset if the files have the same format. For example:</p> <p><pre><code>file_patterns = ['file_1', 'file_2', 'dir_1/*']\nfile_formats = ['recordio', 'recordio', 'sstable']\ntfxio = SomeTFXIO(file_patterns, file_formats)\ndatasets = tfxio.RawRecordTensorFlowDataset(options)\n</code></pre> <code>datasets</code> would result in the following dataset: <code>[ds1, ds2]</code>. Where ds1 iterates over records from 'file_1' and 'file_2', and ds2 iterates over records from files matched by 'dir_1/*'.</p> <p>Example usage: <pre><code>tfxio = SomeTFXIO(file_patterns, file_formats)\nds = tfxio.RawRecordTensorFlowDataset(options=options)\nds = ds.flat_map(lambda x: x)\nrecords = list(ds.as_numpy_iterator())\n# iterating over `records` yields records from the each file in\n# `file_patterns`. See `tf.data.Dataset.list_files` for more information\n# about the order of files when expanding globs.\n</code></pre> Note that we need a flat_map, because <code>RawRecordTensorFlowDataset</code> returns a dataset of datasets.</p> <p>When shuffle=True, then the datasets not deterministically ordered, but the contents of each nested dataset are deterministcally ordered. For example, we may potentially have [ds2, ds1, ds3], where the contents of ds1, ds2, and ds3 are all deterministcally ordered.</p> <p>options: A TensorFlowDatasetOptions object. Not all options will apply.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordTensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Returns a Dataset that contains nested Datasets of raw records.\n\n    May not be implemented for some TFXIOs.\n\n    This should be used when RawTfRecordTFXIO.TensorFlowDataset does not\n    suffice. Namely, if there is some logical grouping of files which we need\n    to perform operations on, without applying the operation to each individual\n    group (i.e. shuffle).\n\n    The returned Dataset object is a dataset of datasets, where each nested\n    dataset is a dataset of serialized records. When shuffle=False (default),\n    the nested datasets are deterministically ordered. Each nested dataset can\n    represent multiple files. The files are merged into one dataset if the files\n    have the same format. For example:\n\n    ```\n    file_patterns = ['file_1', 'file_2', 'dir_1/*']\n    file_formats = ['recordio', 'recordio', 'sstable']\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    datasets = tfxio.RawRecordTensorFlowDataset(options)\n    ```\n    `datasets` would result in the following dataset: `[ds1, ds2]`. Where ds1\n    iterates over records from 'file_1' and 'file_2', and ds2 iterates over\n    records from files matched by 'dir_1/*'.\n\n    Example usage:\n    ```\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    ds = tfxio.RawRecordTensorFlowDataset(options=options)\n    ds = ds.flat_map(lambda x: x)\n    records = list(ds.as_numpy_iterator())\n    # iterating over `records` yields records from the each file in\n    # `file_patterns`. See `tf.data.Dataset.list_files` for more information\n    # about the order of files when expanding globs.\n    ```\n    Note that we need a flat_map, because `RawRecordTensorFlowDataset` returns\n    a dataset of datasets.\n\n    When shuffle=True, then the datasets not deterministically ordered,\n    but the contents of each nested dataset are deterministcally ordered.\n    For example, we may potentially have [ds2, ds1, ds3], where the\n    contents of ds1, ds2, and ds3 are all deterministcally ordered.\n\n    Args:\n    ----\n      options: A TensorFlowDatasetOptions object. Not all options will apply.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.RawRecordToRecordBatch","title":"RawRecordToRecordBatch","text":"<pre><code>RawRecordToRecordBatch(\n    batch_size: Optional[int] = None,\n) -&gt; PTransform\n</code></pre> <p>Returns a PTransform that converts raw records to Arrow RecordBatches.</p> <p>The input PCollection must be from self.RawRecordBeamSource() (also see the documentation for that method).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordToRecordBatch(\n    self, batch_size: Optional[int] = None\n) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that converts raw records to Arrow RecordBatches.\n\n    The input PCollection must be from self.RawRecordBeamSource() (also see\n    the documentation for that method).\n\n    Args:\n    ----\n      batch_size: if not None, the `pa.RecordBatch` produced will be of the\n        specified size. Otherwise it's automatically tuned by Beam.\n    \"\"\"\n\n    @beam.typehints.with_input_types(bytes)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll: beam.pvalue.PCollection):\n        return (\n            pcoll\n            | \"RawRecordToRecordBatch\"\n            &gt;&gt; self._RawRecordToRecordBatchInternal(batch_size)\n            | \"CollectRecordBatchTelemetry\"\n            &gt;&gt; telemetry.ProfileRecordBatches(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.RecordBatches","title":"RecordBatches","text":"<pre><code>RecordBatches(options: RecordBatchesOptions)\n</code></pre> <p>Returns an iterable of record batches.</p> <p>This can be used outside of Apache Beam or TensorFlow to access data.</p> <p>options: An options object for iterating over record batches. Look at     <code>dataset_options.RecordBatchesOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RecordBatches(self, options: dataset_options.RecordBatchesOptions):\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.SupportAttachingRawRecords","title":"SupportAttachingRawRecords","text":"<pre><code>SupportAttachingRawRecords() -&gt; bool\n</code></pre> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def SupportAttachingRawRecords(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter() -&gt; TensorAdapter\n</code></pre> <p>Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapter(self) -&gt; tensor_adapter.TensorAdapter:\n    \"\"\"Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n    return tensor_adapter.TensorAdapter(self.TensorAdapterConfig())\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig() -&gt; TensorAdapterConfig\n</code></pre> <p>Returns the config to initialize a <code>TensorAdapter</code>.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.TensorAdapterConfig--returns","title":"Returns","text":"<p>a <code>TensorAdapterConfig</code> that is the same as what is used to initialize the   <code>TensorAdapter</code> returned by <code>self.TensorAdapter()</code>.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapterConfig(self) -&gt; tensor_adapter.TensorAdapterConfig:\n    \"\"\"Returns the config to initialize a `TensorAdapter`.\n\n    Returns\n    -------\n      a `TensorAdapterConfig` that is the same as what is used to initialize the\n      `TensorAdapter` returned by `self.TensorAdapter()`.\n    \"\"\"\n    return tensor_adapter.TensorAdapterConfig(\n        self.ArrowSchema(), self.TensorRepresentations()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.TensorFlowDataset","title":"TensorFlowDataset","text":"<pre><code>TensorFlowDataset(options: TensorFlowDatasetOptions)\n</code></pre> <p>Returns a tf.data.Dataset of TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>options: an options object for the tf.data.Dataset. Look at     <code>dataset_options.TensorFlowDatasetOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def TensorFlowDataset(self, options: dataset_options.TensorFlowDatasetOptions):\n    raise NotImplementedError(\n        \"TFExampleBeamRecord is unable to provide a TensorFlowDataset \"\n        \"because it does not do I/O\"\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleBeamRecord.TensorRepresentations","title":"TensorRepresentations","text":"<pre><code>TensorRepresentations() -&gt; TensorRepresentations\n</code></pre> <p>Returns the <code>TensorRepresentations</code>.</p> <p>These <code>TensorRepresentation</code>s describe the tensors or composite tensors produced by the <code>TensorAdapter</code> created from <code>self.TensorAdapter()</code> or the tf.data.Dataset created from <code>self.TensorFlowDataset()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time. May raise an error if the tensor representations are invalid.</p> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def TensorRepresentations(self) -&gt; tensor_adapter.TensorRepresentations:\n    return tensor_representation_util.InferTensorRepresentationsFromMixedSchema(\n        self._schema\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord","title":"TFExampleRecord","text":"<pre><code>TFExampleRecord(\n    file_pattern: Union[List[str], str],\n    validate: bool = True,\n    schema: Optional[Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n    telemetry_descriptors: Optional[List[str]] = None,\n)\n</code></pre> <p>               Bases: <code>_TFExampleRecordBase</code></p> <p>TFXIO implementation for tf.Example on TFRecord.</p> <p>Initializes a TFExampleRecord TFXIO.</p> <p>file_pattern: A file glob pattern to read TFRecords from.   validate: Not used. do not set. (not used since post 0.22.1).   schema: A TFMD Schema describing the dataset.   raw_record_column_name: If not None, the generated Arrow RecordBatches     will contain a column of the given name that contains serialized     records.   telemetry_descriptors: A set of descriptors that identify the component     that is instantiating this TFXIO. These will be used to construct the     namespace to contain metrics for profiling and are therefore expected to     be identifiers of the component itself and not individual instances of     source use.</p> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def __init__(\n    self,\n    file_pattern: Union[List[str], str],\n    validate: bool = True,\n    schema: Optional[schema_pb2.Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n    telemetry_descriptors: Optional[List[str]] = None,\n):\n    \"\"\"Initializes a TFExampleRecord TFXIO.\n\n    Args:\n    ----\n      file_pattern: A file glob pattern to read TFRecords from.\n      validate: Not used. do not set. (not used since post 0.22.1).\n      schema: A TFMD Schema describing the dataset.\n      raw_record_column_name: If not None, the generated Arrow RecordBatches\n        will contain a column of the given name that contains serialized\n        records.\n      telemetry_descriptors: A set of descriptors that identify the component\n        that is instantiating this TFXIO. These will be used to construct the\n        namespace to contain metrics for profiling and are therefore expected to\n        be identifiers of the component itself and not individual instances of\n        source use.\n    \"\"\"\n    super().__init__(\n        schema=schema,\n        raw_record_column_name=raw_record_column_name,\n        telemetry_descriptors=telemetry_descriptors,\n        physical_format=\"tfrecords_gzip\",\n    )\n    del validate\n    if not isinstance(file_pattern, list):\n        file_pattern = [file_pattern]\n    assert file_pattern, \"Must provide at least one file pattern.\"\n    self._file_pattern = file_pattern\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.raw_record_column_name","title":"raw_record_column_name  <code>property</code>","text":"<pre><code>raw_record_column_name: Optional[str]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.telemetry_descriptors","title":"telemetry_descriptors  <code>property</code>","text":"<pre><code>telemetry_descriptors: Optional[List[str]]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.ArrowSchema","title":"ArrowSchema","text":"<pre><code>ArrowSchema() -&gt; Schema\n</code></pre> <p>Returns the schema of the <code>RecordBatch</code> produced by <code>self.BeamSource()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def ArrowSchema(self) -&gt; pa.Schema:\n    schema = self._ArrowSchemaNoRawRecordColumn()\n    if self._raw_record_column_name is not None:\n        if schema.get_field_index(self._raw_record_column_name) != -1:\n            raise ValueError(\n                f\"Raw record column name {self._raw_record_column_name} collided with a column in the schema.\"\n            )\n        schema = schema.append(\n            pa.field(self._raw_record_column_name, pa.large_list(pa.large_binary()))\n        )\n    return schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.BeamSource","title":"BeamSource","text":"<pre><code>BeamSource(batch_size: Optional[int] = None) -&gt; PTransform\n</code></pre> <p>Returns a beam <code>PTransform</code> that produces <code>PCollection[pa.RecordBatch]</code>.</p> <p>May NOT raise an error if the TFMD schema was not provided at construction time.</p> <p>If a TFMD schema was provided at construction time, all the <code>pa.RecordBatch</code>es in the result <code>PCollection</code> must be of the same schema returned by <code>self.ArrowSchema</code>. If a TFMD schema was not provided, the <code>pa.RecordBatch</code>es might not be of the same schema (they may contain different numbers of columns).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def BeamSource(self, batch_size: Optional[int] = None) -&gt; beam.PTransform:\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        \"\"\"Converts raw records to RecordBatches.\"\"\"\n        return (\n            pcoll_or_pipeline\n            | \"RawRecordBeamSource\" &gt;&gt; self.RawRecordBeamSource()\n            | \"RawRecordToRecordBatch\" &gt;&gt; self.RawRecordToRecordBatch(batch_size)\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.Project","title":"Project","text":"<pre><code>Project(tensor_names: List[str]) -&gt; TFXIO\n</code></pre> <p>Projects the dataset represented by this TFXIO.</p> <p>A Projected TFXIO: - Only columns needed for given tensor_names are guaranteed to be   produced by <code>self.BeamSource()</code> - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed   to contain only those tensors. - It retains a reference to the very original TFXIO, so its TensorAdapter   knows about the specs of the tensors that would be produced by the   original TensorAdapter. Also see <code>TensorAdapter.OriginalTensorSpec()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>tensor_names: a set of tensor names.</p> <p>A <code>TFXIO</code> instance that is the same as <code>self</code> except that:   - Only columns needed for given tensor_names are guaranteed to be     produced by <code>self.BeamSource()</code>   - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed     to contain only those tensors.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def Project(self, tensor_names: List[str]) -&gt; \"TFXIO\":\n    \"\"\"Projects the dataset represented by this TFXIO.\n\n    A Projected TFXIO:\n    - Only columns needed for given tensor_names are guaranteed to be\n      produced by `self.BeamSource()`\n    - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n      to contain only those tensors.\n    - It retains a reference to the very original TFXIO, so its TensorAdapter\n      knows about the specs of the tensors that would be produced by the\n      original TensorAdapter. Also see `TensorAdapter.OriginalTensorSpec()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      tensor_names: a set of tensor names.\n\n    Returns:\n    -------\n      A `TFXIO` instance that is the same as `self` except that:\n      - Only columns needed for given tensor_names are guaranteed to be\n        produced by `self.BeamSource()`\n      - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n        to contain only those tensors.\n    \"\"\"\n    if isinstance(self, _ProjectedTFXIO):\n        # pylint: disable=protected-access\n        return _ProjectedTFXIO(\n            self.origin, self.projected._ProjectImpl(tensor_names)\n        )\n    return _ProjectedTFXIO(self, self._ProjectImpl(tensor_names))\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.RawRecordBeamSource","title":"RawRecordBeamSource","text":"<pre><code>RawRecordBeamSource() -&gt; PTransform\n</code></pre> <p>Returns a PTransform that produces a PCollection[bytes].</p> <p>Used together with RawRecordToRecordBatch(), it allows getting both the PCollection of the raw records and the PCollection of the RecordBatch from the same source. For example:</p> <p>record_batch = pipeline | tfxio.BeamSource() raw_record = pipeline | tfxio.RawRecordBeamSource()</p> <p>would result in the files being read twice, while the following would only read once:</p> <p>raw_record = pipeline | tfxio.RawRecordBeamSource() record_batch = raw_record | tfxio.RawRecordToRecordBatch()</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordBeamSource(self) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that produces a PCollection[bytes].\n\n    Used together with RawRecordToRecordBatch(), it allows getting both the\n    PCollection of the raw records and the PCollection of the RecordBatch from\n    the same source. For example:\n\n    record_batch = pipeline | tfxio.BeamSource()\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n\n    would result in the files being read twice, while the following would only\n    read once:\n\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n    record_batch = raw_record | tfxio.RawRecordToRecordBatch()\n    \"\"\"\n\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(bytes)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        return (\n            pcoll_or_pipeline\n            | \"ReadRawRecords\" &gt;&gt; self._RawRecordBeamSourceInternal()\n            | \"CollectRawRecordTelemetry\"\n            &gt;&gt; telemetry.ProfileRawRecords(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.RawRecordTensorFlowDataset","title":"RawRecordTensorFlowDataset","text":"<pre><code>RawRecordTensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Returns a Dataset that contains nested Datasets of raw records.</p> <p>May not be implemented for some TFXIOs.</p> <p>This should be used when RawTfRecordTFXIO.TensorFlowDataset does not suffice. Namely, if there is some logical grouping of files which we need to perform operations on, without applying the operation to each individual group (i.e. shuffle).</p> <p>The returned Dataset object is a dataset of datasets, where each nested dataset is a dataset of serialized records. When shuffle=False (default), the nested datasets are deterministically ordered. Each nested dataset can represent multiple files. The files are merged into one dataset if the files have the same format. For example:</p> <p><pre><code>file_patterns = ['file_1', 'file_2', 'dir_1/*']\nfile_formats = ['recordio', 'recordio', 'sstable']\ntfxio = SomeTFXIO(file_patterns, file_formats)\ndatasets = tfxio.RawRecordTensorFlowDataset(options)\n</code></pre> <code>datasets</code> would result in the following dataset: <code>[ds1, ds2]</code>. Where ds1 iterates over records from 'file_1' and 'file_2', and ds2 iterates over records from files matched by 'dir_1/*'.</p> <p>Example usage: <pre><code>tfxio = SomeTFXIO(file_patterns, file_formats)\nds = tfxio.RawRecordTensorFlowDataset(options=options)\nds = ds.flat_map(lambda x: x)\nrecords = list(ds.as_numpy_iterator())\n# iterating over `records` yields records from the each file in\n# `file_patterns`. See `tf.data.Dataset.list_files` for more information\n# about the order of files when expanding globs.\n</code></pre> Note that we need a flat_map, because <code>RawRecordTensorFlowDataset</code> returns a dataset of datasets.</p> <p>When shuffle=True, then the datasets not deterministically ordered, but the contents of each nested dataset are deterministcally ordered. For example, we may potentially have [ds2, ds1, ds3], where the contents of ds1, ds2, and ds3 are all deterministcally ordered.</p> <p>options: A TensorFlowDatasetOptions object. Not all options will apply.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordTensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Returns a Dataset that contains nested Datasets of raw records.\n\n    May not be implemented for some TFXIOs.\n\n    This should be used when RawTfRecordTFXIO.TensorFlowDataset does not\n    suffice. Namely, if there is some logical grouping of files which we need\n    to perform operations on, without applying the operation to each individual\n    group (i.e. shuffle).\n\n    The returned Dataset object is a dataset of datasets, where each nested\n    dataset is a dataset of serialized records. When shuffle=False (default),\n    the nested datasets are deterministically ordered. Each nested dataset can\n    represent multiple files. The files are merged into one dataset if the files\n    have the same format. For example:\n\n    ```\n    file_patterns = ['file_1', 'file_2', 'dir_1/*']\n    file_formats = ['recordio', 'recordio', 'sstable']\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    datasets = tfxio.RawRecordTensorFlowDataset(options)\n    ```\n    `datasets` would result in the following dataset: `[ds1, ds2]`. Where ds1\n    iterates over records from 'file_1' and 'file_2', and ds2 iterates over\n    records from files matched by 'dir_1/*'.\n\n    Example usage:\n    ```\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    ds = tfxio.RawRecordTensorFlowDataset(options=options)\n    ds = ds.flat_map(lambda x: x)\n    records = list(ds.as_numpy_iterator())\n    # iterating over `records` yields records from the each file in\n    # `file_patterns`. See `tf.data.Dataset.list_files` for more information\n    # about the order of files when expanding globs.\n    ```\n    Note that we need a flat_map, because `RawRecordTensorFlowDataset` returns\n    a dataset of datasets.\n\n    When shuffle=True, then the datasets not deterministically ordered,\n    but the contents of each nested dataset are deterministcally ordered.\n    For example, we may potentially have [ds2, ds1, ds3], where the\n    contents of ds1, ds2, and ds3 are all deterministcally ordered.\n\n    Args:\n    ----\n      options: A TensorFlowDatasetOptions object. Not all options will apply.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.RawRecordToRecordBatch","title":"RawRecordToRecordBatch","text":"<pre><code>RawRecordToRecordBatch(\n    batch_size: Optional[int] = None,\n) -&gt; PTransform\n</code></pre> <p>Returns a PTransform that converts raw records to Arrow RecordBatches.</p> <p>The input PCollection must be from self.RawRecordBeamSource() (also see the documentation for that method).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordToRecordBatch(\n    self, batch_size: Optional[int] = None\n) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that converts raw records to Arrow RecordBatches.\n\n    The input PCollection must be from self.RawRecordBeamSource() (also see\n    the documentation for that method).\n\n    Args:\n    ----\n      batch_size: if not None, the `pa.RecordBatch` produced will be of the\n        specified size. Otherwise it's automatically tuned by Beam.\n    \"\"\"\n\n    @beam.typehints.with_input_types(bytes)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll: beam.pvalue.PCollection):\n        return (\n            pcoll\n            | \"RawRecordToRecordBatch\"\n            &gt;&gt; self._RawRecordToRecordBatchInternal(batch_size)\n            | \"CollectRecordBatchTelemetry\"\n            &gt;&gt; telemetry.ProfileRecordBatches(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.RecordBatches","title":"RecordBatches","text":"<pre><code>RecordBatches(\n    options: RecordBatchesOptions,\n) -&gt; Iterator[RecordBatch]\n</code></pre> <p>Returns an iterable of record batches.</p> <p>This can be used outside of Apache Beam or TensorFlow to access data.</p> <p>options: An options object for iterating over record batches. Look at     <code>dataset_options.RecordBatchesOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def RecordBatches(\n    self, options: dataset_options.RecordBatchesOptions\n) -&gt; Iterator[pa.RecordBatch]:\n    dataset = dataset_util.make_tf_record_dataset(\n        self._file_pattern,\n        options.batch_size,\n        options.drop_final_batch,\n        options.num_epochs,\n        options.shuffle,\n        options.shuffle_buffer_size,\n        options.shuffle_seed,\n    )\n\n    decoder = example_coder.ExamplesToRecordBatchDecoder(\n        self._schema.SerializeToString() if self._schema is not None else None\n    )\n    for examples in dataset.as_numpy_iterator():\n        decoded = decoder.DecodeBatch(examples)\n        if self._raw_record_column_name is None:\n            yield decoded\n        else:\n            yield record_based_tfxio.AppendRawRecordColumn(\n                decoded, self._raw_record_column_name, examples.tolist()\n            )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.SupportAttachingRawRecords","title":"SupportAttachingRawRecords","text":"<pre><code>SupportAttachingRawRecords() -&gt; bool\n</code></pre> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def SupportAttachingRawRecords(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter() -&gt; TensorAdapter\n</code></pre> <p>Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapter(self) -&gt; tensor_adapter.TensorAdapter:\n    \"\"\"Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n    return tensor_adapter.TensorAdapter(self.TensorAdapterConfig())\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig() -&gt; TensorAdapterConfig\n</code></pre> <p>Returns the config to initialize a <code>TensorAdapter</code>.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.TensorAdapterConfig--returns","title":"Returns","text":"<p>a <code>TensorAdapterConfig</code> that is the same as what is used to initialize the   <code>TensorAdapter</code> returned by <code>self.TensorAdapter()</code>.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapterConfig(self) -&gt; tensor_adapter.TensorAdapterConfig:\n    \"\"\"Returns the config to initialize a `TensorAdapter`.\n\n    Returns\n    -------\n      a `TensorAdapterConfig` that is the same as what is used to initialize the\n      `TensorAdapter` returned by `self.TensorAdapter()`.\n    \"\"\"\n    return tensor_adapter.TensorAdapterConfig(\n        self.ArrowSchema(), self.TensorRepresentations()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.TensorFlowDataset","title":"TensorFlowDataset","text":"<pre><code>TensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Creates a TFRecordDataset that yields Tensors.</p> <p>The serialized tf.Examples are parsed by <code>tf.io.parse_example</code> to create Tensors.</p> <p>See base class (tfxio.TFXIO) for more details.</p> <p>options: an options object for the tf.data.Dataset. See     <code>dataset_options.TensorFlowDatasetOptions</code> for more details.</p> <p>A dataset of <code>dict</code> elements, (or a tuple of <code>dict</code> elements and label).   Each <code>dict</code> maps feature keys to <code>Tensor</code>, <code>SparseTensor</code>, or   <code>RaggedTensor</code> objects.</p> <p>ValueError: if there is something wrong with the tensor_representation.</p> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def TensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Creates a TFRecordDataset that yields Tensors.\n\n    The serialized tf.Examples are parsed by `tf.io.parse_example` to create\n    Tensors.\n\n    See base class (tfxio.TFXIO) for more details.\n\n    Args:\n    ----\n      options: an options object for the tf.data.Dataset. See\n        `dataset_options.TensorFlowDatasetOptions` for more details.\n\n    Returns:\n    -------\n      A dataset of `dict` elements, (or a tuple of `dict` elements and label).\n      Each `dict` maps feature keys to `Tensor`, `SparseTensor`, or\n      `RaggedTensor` objects.\n\n    Raises:\n    ------\n      ValueError: if there is something wrong with the tensor_representation.\n    \"\"\"\n    (tf_example_parser_config, feature_name_to_tensor_name) = (\n        self._GetTfExampleParserConfig()\n    )\n\n    file_pattern = tf.convert_to_tensor(self._file_pattern)\n    dataset = dataset_util.make_tf_record_dataset(\n        file_pattern,\n        batch_size=options.batch_size,\n        num_epochs=options.num_epochs,\n        shuffle=options.shuffle,\n        shuffle_buffer_size=options.shuffle_buffer_size,\n        shuffle_seed=options.shuffle_seed,\n        reader_num_threads=options.reader_num_threads,\n        drop_final_batch=options.drop_final_batch,\n    )\n\n    # Parse `Example` tensors to a dictionary of `Feature` tensors.\n    dataset = dataset.apply(\n        tf.data.experimental.parse_example_dataset(tf_example_parser_config)\n    )\n\n    dataset = dataset.map(\n        lambda x: self._RenameFeatures(x, feature_name_to_tensor_name)\n    )\n\n    label_key = options.label_key\n    if label_key is not None:\n        dataset = self._PopLabelFeatureFromDataset(dataset, label_key)\n\n    return dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFExampleRecord.TensorRepresentations","title":"TensorRepresentations","text":"<pre><code>TensorRepresentations() -&gt; TensorRepresentations\n</code></pre> <p>Returns the <code>TensorRepresentations</code>.</p> <p>These <code>TensorRepresentation</code>s describe the tensors or composite tensors produced by the <code>TensorAdapter</code> created from <code>self.TensorAdapter()</code> or the tf.data.Dataset created from <code>self.TensorFlowDataset()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time. May raise an error if the tensor representations are invalid.</p> Source code in <code>tfx_bsl/tfxio/tf_example_record.py</code> <pre><code>def TensorRepresentations(self) -&gt; tensor_adapter.TensorRepresentations:\n    return tensor_representation_util.InferTensorRepresentationsFromMixedSchema(\n        self._schema\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder","title":"TFGraphRecordDecoder","text":"<p>Base class for decoders that turns a list of bytes to (composite) tensors.</p> <p>Sub-classes must implement <code>decode_record()</code> (see its docstring for requirements).</p> <p>Decoder instances can be saved as a SavedModel by <code>save_decoder()</code>. The SavedModel can be loaded back by <code>load_decoder()</code>. However, the loaded decoder will always be of the type <code>LoadedDecoder</code> and only have the public interfaces listed in this base class available.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder.record_index_tensor_name","title":"record_index_tensor_name  <code>property</code>","text":"<pre><code>record_index_tensor_name: Optional[str]\n</code></pre> <p>The name of the tensor indicating which record a slice is from.</p> <p>The decoded tensors are batch-aligned among themselves, but they don't necessarily have to be batch-aligned with the input records. If not, sub-classes should implement this method to tie the batch dimension with the input record.</p> <p>The record index tensor must be a SparseTensor or a RaggedTensor of integral type, and must be 2-D and must not contain \"missing\" values.</p> <p>A record index tensor like the following: [[0], [0], [2]] means that of 3 \"rows\" in the output \"batch\", the first two rows came from the first record, and the 3rd row came from the third record.</p> <p>The name must not be an empty string.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder.record_index_tensor_name--returns","title":"Returns","text":"<p>The name of the record index tensor.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder.decode_record","title":"decode_record  <code>abstractmethod</code>","text":"<pre><code>decode_record(records: Tensor) -&gt; Dict[str, TensorAlike]\n</code></pre> <p>Sub-classes should implement this.</p> <p>Implementations must use TF ops to derive the result (composite) tensors, as this function will be traced and become a tf.function (thus a TF Graph). Note that autograph is not enabled in such tracing, which means any python control flow / loops will not be converted to TF cond / loops automatically.</p> <p>The returned tensors must be batch-aligned (i.e. they should be at least of rank 1, and their outer-most dimensions must be of the same size). They do not have to be batch-aligned with the input tensor, but if that's the case, an additional tensor must be provided among the results, to indicate which input record a \"row\" in the output batch comes from. See <code>record_index_tensor_name</code> for more details.</p> <p>records: a 1-D string tensor that contains the records to be decoded.</p> <p>A dict of (composite) tensors.</p> Source code in <code>tfx_bsl/coders/tf_graph_record_decoder.py</code> <pre><code>@abc.abstractmethod\ndef decode_record(self, records: tf.Tensor) -&gt; Dict[str, TensorAlike]:\n    \"\"\"Sub-classes should implement this.\n\n    Implementations must use TF ops to derive the result (composite) tensors, as\n    this function will be traced and become a tf.function (thus a TF Graph).\n    Note that autograph is not enabled in such tracing, which means any python\n    control flow / loops will not be converted to TF cond / loops automatically.\n\n    The returned tensors must be batch-aligned (i.e. they should be at least\n    of rank 1, and their outer-most dimensions must be of the same size). They\n    do not have to be batch-aligned with the input tensor, but if that's the\n    case, an additional tensor must be provided among the results, to indicate\n    which input record a \"row\" in the output batch comes from. See\n    `record_index_tensor_name` for more details.\n\n    Args:\n    ----\n      records: a 1-D string tensor that contains the records to be decoded.\n\n    Returns:\n    -------\n      A dict of (composite) tensors.\n    \"\"\"\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder.output_type_specs","title":"output_type_specs","text":"<pre><code>output_type_specs() -&gt; Dict[str, TypeSpec]\n</code></pre> <p>Returns the tf.TypeSpecs of the decoded tensors.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder.output_type_specs--returns","title":"Returns","text":"<p>A dict whose keys are the same as keys of the dict returned by   <code>decode_record()</code> and values are the tf.TypeSpec of the corresponding   (composite) tensor.</p> Source code in <code>tfx_bsl/coders/tf_graph_record_decoder.py</code> <pre><code>def output_type_specs(self) -&gt; Dict[str, tf.TypeSpec]:\n    \"\"\"Returns the tf.TypeSpecs of the decoded tensors.\n\n    Returns\n    -------\n      A dict whose keys are the same as keys of the dict returned by\n      `decode_record()` and values are the tf.TypeSpec of the corresponding\n      (composite) tensor.\n    \"\"\"\n    return {\n        k: tf.type_spec_from_value(v)\n        for k, v in self._make_concrete_decode_function().structured_outputs.items()\n    }\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFGraphRecordDecoder.save","title":"save","text":"<pre><code>save(path: str) -&gt; None\n</code></pre> <p>Saves this TFGraphRecordDecoder to a SavedModel at <code>path</code>.</p> <p>This functions the same as <code>tf_graph_record_decoder.save_decoder()</code>. This is provided purely for convenience, and should not impact the actual saved model, since only the <code>tf.function</code> from <code>_make_concrete_decode_function</code> is saved.</p> <p>path: The path to where the saved_model is saved.</p> Source code in <code>tfx_bsl/coders/tf_graph_record_decoder.py</code> <pre><code>def save(self, path: str) -&gt; None:\n    \"\"\"Saves this TFGraphRecordDecoder to a SavedModel at `path`.\n\n    This functions the same as `tf_graph_record_decoder.save_decoder()`. This is\n    provided purely for convenience, and should not impact the actual saved\n    model, since only the `tf.function` from `_make_concrete_decode_function` is\n    saved.\n\n    Args:\n    ----\n      path: The path to where the saved_model is saved.\n    \"\"\"\n    save_decoder(self, path)\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord","title":"TFSequenceExampleBeamRecord","text":"<pre><code>TFSequenceExampleBeamRecord(\n    physical_format: str,\n    telemetry_descriptors: List[str],\n    schema: Optional[Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>_TFSequenceExampleRecordBase</code></p> <p>TFXIO implementation for serialized tf.SequenceExamples in pcoll[bytes].</p> <p>This is a special TFXIO that does not actually do I/O -- it relies on the caller to prepare a PCollection of bytes (serialized tf.SequenceExamples).</p> <p>Initializer.</p> <p>physical_format: The physical format that describes where the input     pcoll[bytes] comes from. Used for telemetry purposes. Examples: \"text\",     \"tfrecord\".   telemetry_descriptors: A set of descriptors that identify the component     that is instantiating this TFXIO. These will be used to construct the     namespace to contain metrics for profiling and are therefore expected to     be identifiers of the component itself and not individual instances of     source use.   schema: A TFMD Schema describing the dataset.   raw_record_column_name: If not None, the generated Arrow RecordBatches     will contain a column of the given name that contains serialized     records.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def __init__(\n    self,\n    physical_format: str,\n    telemetry_descriptors: List[str],\n    schema: Optional[schema_pb2.Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n):\n    \"\"\"Initializer.\n\n    Args:\n    ----\n      physical_format: The physical format that describes where the input\n        pcoll[bytes] comes from. Used for telemetry purposes. Examples: \"text\",\n        \"tfrecord\".\n      telemetry_descriptors: A set of descriptors that identify the component\n        that is instantiating this TFXIO. These will be used to construct the\n        namespace to contain metrics for profiling and are therefore expected to\n        be identifiers of the component itself and not individual instances of\n        source use.\n      schema: A TFMD Schema describing the dataset.\n      raw_record_column_name: If not None, the generated Arrow RecordBatches\n        will contain a column of the given name that contains serialized\n        records.\n    \"\"\"\n    super().__init__(\n        schema=schema,\n        raw_record_column_name=raw_record_column_name,\n        telemetry_descriptors=telemetry_descriptors,\n        physical_format=physical_format,\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.raw_record_column_name","title":"raw_record_column_name  <code>property</code>","text":"<pre><code>raw_record_column_name: Optional[str]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.telemetry_descriptors","title":"telemetry_descriptors  <code>property</code>","text":"<pre><code>telemetry_descriptors: Optional[List[str]]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.ArrowSchema","title":"ArrowSchema","text":"<pre><code>ArrowSchema() -&gt; Schema\n</code></pre> <p>Returns the schema of the <code>RecordBatch</code> produced by <code>self.BeamSource()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def ArrowSchema(self) -&gt; pa.Schema:\n    schema = self._ArrowSchemaNoRawRecordColumn()\n    if self._raw_record_column_name is not None:\n        if schema.get_field_index(self._raw_record_column_name) != -1:\n            raise ValueError(\n                f\"Raw record column name {self._raw_record_column_name} collided with a column in the schema.\"\n            )\n        schema = schema.append(\n            pa.field(self._raw_record_column_name, pa.large_list(pa.large_binary()))\n        )\n    return schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.BeamSource","title":"BeamSource","text":"<pre><code>BeamSource(batch_size: Optional[int] = None) -&gt; PTransform\n</code></pre> <p>Returns a beam <code>PTransform</code> that produces <code>PCollection[pa.RecordBatch]</code>.</p> <p>May NOT raise an error if the TFMD schema was not provided at construction time.</p> <p>If a TFMD schema was provided at construction time, all the <code>pa.RecordBatch</code>es in the result <code>PCollection</code> must be of the same schema returned by <code>self.ArrowSchema</code>. If a TFMD schema was not provided, the <code>pa.RecordBatch</code>es might not be of the same schema (they may contain different numbers of columns).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def BeamSource(self, batch_size: Optional[int] = None) -&gt; beam.PTransform:\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        \"\"\"Converts raw records to RecordBatches.\"\"\"\n        return (\n            pcoll_or_pipeline\n            | \"RawRecordBeamSource\" &gt;&gt; self.RawRecordBeamSource()\n            | \"RawRecordToRecordBatch\" &gt;&gt; self.RawRecordToRecordBatch(batch_size)\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.Project","title":"Project","text":"<pre><code>Project(tensor_names: List[str]) -&gt; TFXIO\n</code></pre> <p>Projects the dataset represented by this TFXIO.</p> <p>A Projected TFXIO: - Only columns needed for given tensor_names are guaranteed to be   produced by <code>self.BeamSource()</code> - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed   to contain only those tensors. - It retains a reference to the very original TFXIO, so its TensorAdapter   knows about the specs of the tensors that would be produced by the   original TensorAdapter. Also see <code>TensorAdapter.OriginalTensorSpec()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>tensor_names: a set of tensor names.</p> <p>A <code>TFXIO</code> instance that is the same as <code>self</code> except that:   - Only columns needed for given tensor_names are guaranteed to be     produced by <code>self.BeamSource()</code>   - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed     to contain only those tensors.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def Project(self, tensor_names: List[str]) -&gt; \"TFXIO\":\n    \"\"\"Projects the dataset represented by this TFXIO.\n\n    A Projected TFXIO:\n    - Only columns needed for given tensor_names are guaranteed to be\n      produced by `self.BeamSource()`\n    - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n      to contain only those tensors.\n    - It retains a reference to the very original TFXIO, so its TensorAdapter\n      knows about the specs of the tensors that would be produced by the\n      original TensorAdapter. Also see `TensorAdapter.OriginalTensorSpec()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      tensor_names: a set of tensor names.\n\n    Returns:\n    -------\n      A `TFXIO` instance that is the same as `self` except that:\n      - Only columns needed for given tensor_names are guaranteed to be\n        produced by `self.BeamSource()`\n      - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n        to contain only those tensors.\n    \"\"\"\n    if isinstance(self, _ProjectedTFXIO):\n        # pylint: disable=protected-access\n        return _ProjectedTFXIO(\n            self.origin, self.projected._ProjectImpl(tensor_names)\n        )\n    return _ProjectedTFXIO(self, self._ProjectImpl(tensor_names))\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.RawRecordBeamSource","title":"RawRecordBeamSource","text":"<pre><code>RawRecordBeamSource() -&gt; PTransform\n</code></pre> <p>Returns a PTransform that produces a PCollection[bytes].</p> <p>Used together with RawRecordToRecordBatch(), it allows getting both the PCollection of the raw records and the PCollection of the RecordBatch from the same source. For example:</p> <p>record_batch = pipeline | tfxio.BeamSource() raw_record = pipeline | tfxio.RawRecordBeamSource()</p> <p>would result in the files being read twice, while the following would only read once:</p> <p>raw_record = pipeline | tfxio.RawRecordBeamSource() record_batch = raw_record | tfxio.RawRecordToRecordBatch()</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordBeamSource(self) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that produces a PCollection[bytes].\n\n    Used together with RawRecordToRecordBatch(), it allows getting both the\n    PCollection of the raw records and the PCollection of the RecordBatch from\n    the same source. For example:\n\n    record_batch = pipeline | tfxio.BeamSource()\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n\n    would result in the files being read twice, while the following would only\n    read once:\n\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n    record_batch = raw_record | tfxio.RawRecordToRecordBatch()\n    \"\"\"\n\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(bytes)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        return (\n            pcoll_or_pipeline\n            | \"ReadRawRecords\" &gt;&gt; self._RawRecordBeamSourceInternal()\n            | \"CollectRawRecordTelemetry\"\n            &gt;&gt; telemetry.ProfileRawRecords(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.RawRecordTensorFlowDataset","title":"RawRecordTensorFlowDataset","text":"<pre><code>RawRecordTensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Returns a Dataset that contains nested Datasets of raw records.</p> <p>May not be implemented for some TFXIOs.</p> <p>This should be used when RawTfRecordTFXIO.TensorFlowDataset does not suffice. Namely, if there is some logical grouping of files which we need to perform operations on, without applying the operation to each individual group (i.e. shuffle).</p> <p>The returned Dataset object is a dataset of datasets, where each nested dataset is a dataset of serialized records. When shuffle=False (default), the nested datasets are deterministically ordered. Each nested dataset can represent multiple files. The files are merged into one dataset if the files have the same format. For example:</p> <p><pre><code>file_patterns = ['file_1', 'file_2', 'dir_1/*']\nfile_formats = ['recordio', 'recordio', 'sstable']\ntfxio = SomeTFXIO(file_patterns, file_formats)\ndatasets = tfxio.RawRecordTensorFlowDataset(options)\n</code></pre> <code>datasets</code> would result in the following dataset: <code>[ds1, ds2]</code>. Where ds1 iterates over records from 'file_1' and 'file_2', and ds2 iterates over records from files matched by 'dir_1/*'.</p> <p>Example usage: <pre><code>tfxio = SomeTFXIO(file_patterns, file_formats)\nds = tfxio.RawRecordTensorFlowDataset(options=options)\nds = ds.flat_map(lambda x: x)\nrecords = list(ds.as_numpy_iterator())\n# iterating over `records` yields records from the each file in\n# `file_patterns`. See `tf.data.Dataset.list_files` for more information\n# about the order of files when expanding globs.\n</code></pre> Note that we need a flat_map, because <code>RawRecordTensorFlowDataset</code> returns a dataset of datasets.</p> <p>When shuffle=True, then the datasets not deterministically ordered, but the contents of each nested dataset are deterministcally ordered. For example, we may potentially have [ds2, ds1, ds3], where the contents of ds1, ds2, and ds3 are all deterministcally ordered.</p> <p>options: A TensorFlowDatasetOptions object. Not all options will apply.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordTensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Returns a Dataset that contains nested Datasets of raw records.\n\n    May not be implemented for some TFXIOs.\n\n    This should be used when RawTfRecordTFXIO.TensorFlowDataset does not\n    suffice. Namely, if there is some logical grouping of files which we need\n    to perform operations on, without applying the operation to each individual\n    group (i.e. shuffle).\n\n    The returned Dataset object is a dataset of datasets, where each nested\n    dataset is a dataset of serialized records. When shuffle=False (default),\n    the nested datasets are deterministically ordered. Each nested dataset can\n    represent multiple files. The files are merged into one dataset if the files\n    have the same format. For example:\n\n    ```\n    file_patterns = ['file_1', 'file_2', 'dir_1/*']\n    file_formats = ['recordio', 'recordio', 'sstable']\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    datasets = tfxio.RawRecordTensorFlowDataset(options)\n    ```\n    `datasets` would result in the following dataset: `[ds1, ds2]`. Where ds1\n    iterates over records from 'file_1' and 'file_2', and ds2 iterates over\n    records from files matched by 'dir_1/*'.\n\n    Example usage:\n    ```\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    ds = tfxio.RawRecordTensorFlowDataset(options=options)\n    ds = ds.flat_map(lambda x: x)\n    records = list(ds.as_numpy_iterator())\n    # iterating over `records` yields records from the each file in\n    # `file_patterns`. See `tf.data.Dataset.list_files` for more information\n    # about the order of files when expanding globs.\n    ```\n    Note that we need a flat_map, because `RawRecordTensorFlowDataset` returns\n    a dataset of datasets.\n\n    When shuffle=True, then the datasets not deterministically ordered,\n    but the contents of each nested dataset are deterministcally ordered.\n    For example, we may potentially have [ds2, ds1, ds3], where the\n    contents of ds1, ds2, and ds3 are all deterministcally ordered.\n\n    Args:\n    ----\n      options: A TensorFlowDatasetOptions object. Not all options will apply.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.RawRecordToRecordBatch","title":"RawRecordToRecordBatch","text":"<pre><code>RawRecordToRecordBatch(\n    batch_size: Optional[int] = None,\n) -&gt; PTransform\n</code></pre> <p>Returns a PTransform that converts raw records to Arrow RecordBatches.</p> <p>The input PCollection must be from self.RawRecordBeamSource() (also see the documentation for that method).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordToRecordBatch(\n    self, batch_size: Optional[int] = None\n) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that converts raw records to Arrow RecordBatches.\n\n    The input PCollection must be from self.RawRecordBeamSource() (also see\n    the documentation for that method).\n\n    Args:\n    ----\n      batch_size: if not None, the `pa.RecordBatch` produced will be of the\n        specified size. Otherwise it's automatically tuned by Beam.\n    \"\"\"\n\n    @beam.typehints.with_input_types(bytes)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll: beam.pvalue.PCollection):\n        return (\n            pcoll\n            | \"RawRecordToRecordBatch\"\n            &gt;&gt; self._RawRecordToRecordBatchInternal(batch_size)\n            | \"CollectRecordBatchTelemetry\"\n            &gt;&gt; telemetry.ProfileRecordBatches(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.RecordBatches","title":"RecordBatches","text":"<pre><code>RecordBatches(options: RecordBatchesOptions)\n</code></pre> <p>Returns an iterable of record batches.</p> <p>This can be used outside of Apache Beam or TensorFlow to access data.</p> <p>options: An options object for iterating over record batches. Look at     <code>dataset_options.RecordBatchesOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def RecordBatches(self, options: dataset_options.RecordBatchesOptions):\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.SupportAttachingRawRecords","title":"SupportAttachingRawRecords","text":"<pre><code>SupportAttachingRawRecords() -&gt; bool\n</code></pre> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def SupportAttachingRawRecords(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter() -&gt; TensorAdapter\n</code></pre> <p>Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapter(self) -&gt; tensor_adapter.TensorAdapter:\n    \"\"\"Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n    return tensor_adapter.TensorAdapter(self.TensorAdapterConfig())\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig() -&gt; TensorAdapterConfig\n</code></pre> <p>Returns the config to initialize a <code>TensorAdapter</code>.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.TensorAdapterConfig--returns","title":"Returns","text":"<p>a <code>TensorAdapterConfig</code> that is the same as what is used to initialize the   <code>TensorAdapter</code> returned by <code>self.TensorAdapter()</code>.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapterConfig(self) -&gt; tensor_adapter.TensorAdapterConfig:\n    \"\"\"Returns the config to initialize a `TensorAdapter`.\n\n    Returns\n    -------\n      a `TensorAdapterConfig` that is the same as what is used to initialize the\n      `TensorAdapter` returned by `self.TensorAdapter()`.\n    \"\"\"\n    return tensor_adapter.TensorAdapterConfig(\n        self.ArrowSchema(), self.TensorRepresentations()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.TensorFlowDataset","title":"TensorFlowDataset","text":"<pre><code>TensorFlowDataset(options: TensorFlowDatasetOptions)\n</code></pre> <p>Returns a tf.data.Dataset of TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>options: an options object for the tf.data.Dataset. Look at     <code>dataset_options.TensorFlowDatasetOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def TensorFlowDataset(self, options: dataset_options.TensorFlowDatasetOptions):\n    raise NotImplementedError(\n        \"TFExampleBeamRecord is unable to provide a TensorFlowDataset \"\n        \"because it does not do I/O\"\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleBeamRecord.TensorRepresentations","title":"TensorRepresentations","text":"<pre><code>TensorRepresentations() -&gt; TensorRepresentations\n</code></pre> <p>Returns the <code>TensorRepresentations</code>.</p> <p>These <code>TensorRepresentation</code>s describe the tensors or composite tensors produced by the <code>TensorAdapter</code> created from <code>self.TensorAdapter()</code> or the tf.data.Dataset created from <code>self.TensorFlowDataset()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time. May raise an error if the tensor representations are invalid.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def TensorRepresentations(self) -&gt; tensor_adapter.TensorRepresentations:\n    return tensor_representation_util.InferTensorRepresentationsFromMixedSchema(\n        self._schema\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord","title":"TFSequenceExampleRecord","text":"<pre><code>TFSequenceExampleRecord(\n    file_pattern: Union[List[str], str],\n    telemetry_descriptors: List[str],\n    validate: bool = True,\n    schema: Optional[Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n)\n</code></pre> <p>               Bases: <code>_TFSequenceExampleRecordBase</code></p> <p>TFXIO implementation for tf.SequenceExample on TFRecord.</p> <p>Initializes a TFSequenceExampleRecord TFXIO.</p> <p>file_pattern: One or a list of glob patterns. If a list, must not be     empty.   telemetry_descriptors: A set of descriptors that identify the component     that is instantiating this TFXIO. These will be used to construct the     namespace to contain metrics for profiling and are therefore expected to     be identifiers of the component itself and not individual instances of     source use.   validate: Not used. do not set. (not used since post 0.22.1).   schema: A TFMD Schema describing the dataset.   raw_record_column_name: If not None, the generated Arrow RecordBatches     will contain a column of the given name that contains serialized     records.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def __init__(\n    self,\n    file_pattern: Union[List[str], str],\n    telemetry_descriptors: List[str],\n    validate: bool = True,\n    schema: Optional[schema_pb2.Schema] = None,\n    raw_record_column_name: Optional[str] = None,\n):\n    \"\"\"Initializes a TFSequenceExampleRecord TFXIO.\n\n    Args:\n    ----\n      file_pattern: One or a list of glob patterns. If a list, must not be\n        empty.\n      telemetry_descriptors: A set of descriptors that identify the component\n        that is instantiating this TFXIO. These will be used to construct the\n        namespace to contain metrics for profiling and are therefore expected to\n        be identifiers of the component itself and not individual instances of\n        source use.\n      validate: Not used. do not set. (not used since post 0.22.1).\n      schema: A TFMD Schema describing the dataset.\n      raw_record_column_name: If not None, the generated Arrow RecordBatches\n        will contain a column of the given name that contains serialized\n        records.\n    \"\"\"\n    super().__init__(\n        schema=schema,\n        raw_record_column_name=raw_record_column_name,\n        telemetry_descriptors=telemetry_descriptors,\n        physical_format=\"tfrecords_gzip\",\n    )\n    del validate\n    if not isinstance(file_pattern, list):\n        file_pattern = [file_pattern]\n    assert file_pattern, \"Must provide at least one file pattern.\"\n    self._file_pattern = file_pattern\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.raw_record_column_name","title":"raw_record_column_name  <code>property</code>","text":"<pre><code>raw_record_column_name: Optional[str]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.schema","title":"schema  <code>property</code>","text":"<pre><code>schema: Schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.telemetry_descriptors","title":"telemetry_descriptors  <code>property</code>","text":"<pre><code>telemetry_descriptors: Optional[List[str]]\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.ArrowSchema","title":"ArrowSchema","text":"<pre><code>ArrowSchema() -&gt; Schema\n</code></pre> <p>Returns the schema of the <code>RecordBatch</code> produced by <code>self.BeamSource()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def ArrowSchema(self) -&gt; pa.Schema:\n    schema = self._ArrowSchemaNoRawRecordColumn()\n    if self._raw_record_column_name is not None:\n        if schema.get_field_index(self._raw_record_column_name) != -1:\n            raise ValueError(\n                f\"Raw record column name {self._raw_record_column_name} collided with a column in the schema.\"\n            )\n        schema = schema.append(\n            pa.field(self._raw_record_column_name, pa.large_list(pa.large_binary()))\n        )\n    return schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.BeamSource","title":"BeamSource","text":"<pre><code>BeamSource(batch_size: Optional[int] = None) -&gt; PTransform\n</code></pre> <p>Returns a beam <code>PTransform</code> that produces <code>PCollection[pa.RecordBatch]</code>.</p> <p>May NOT raise an error if the TFMD schema was not provided at construction time.</p> <p>If a TFMD schema was provided at construction time, all the <code>pa.RecordBatch</code>es in the result <code>PCollection</code> must be of the same schema returned by <code>self.ArrowSchema</code>. If a TFMD schema was not provided, the <code>pa.RecordBatch</code>es might not be of the same schema (they may contain different numbers of columns).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def BeamSource(self, batch_size: Optional[int] = None) -&gt; beam.PTransform:\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        \"\"\"Converts raw records to RecordBatches.\"\"\"\n        return (\n            pcoll_or_pipeline\n            | \"RawRecordBeamSource\" &gt;&gt; self.RawRecordBeamSource()\n            | \"RawRecordToRecordBatch\" &gt;&gt; self.RawRecordToRecordBatch(batch_size)\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.Project","title":"Project","text":"<pre><code>Project(tensor_names: List[str]) -&gt; TFXIO\n</code></pre> <p>Projects the dataset represented by this TFXIO.</p> <p>A Projected TFXIO: - Only columns needed for given tensor_names are guaranteed to be   produced by <code>self.BeamSource()</code> - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed   to contain only those tensors. - It retains a reference to the very original TFXIO, so its TensorAdapter   knows about the specs of the tensors that would be produced by the   original TensorAdapter. Also see <code>TensorAdapter.OriginalTensorSpec()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>tensor_names: a set of tensor names.</p> <p>A <code>TFXIO</code> instance that is the same as <code>self</code> except that:   - Only columns needed for given tensor_names are guaranteed to be     produced by <code>self.BeamSource()</code>   - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed     to contain only those tensors.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def Project(self, tensor_names: List[str]) -&gt; \"TFXIO\":\n    \"\"\"Projects the dataset represented by this TFXIO.\n\n    A Projected TFXIO:\n    - Only columns needed for given tensor_names are guaranteed to be\n      produced by `self.BeamSource()`\n    - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n      to contain only those tensors.\n    - It retains a reference to the very original TFXIO, so its TensorAdapter\n      knows about the specs of the tensors that would be produced by the\n      original TensorAdapter. Also see `TensorAdapter.OriginalTensorSpec()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      tensor_names: a set of tensor names.\n\n    Returns:\n    -------\n      A `TFXIO` instance that is the same as `self` except that:\n      - Only columns needed for given tensor_names are guaranteed to be\n        produced by `self.BeamSource()`\n      - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n        to contain only those tensors.\n    \"\"\"\n    if isinstance(self, _ProjectedTFXIO):\n        # pylint: disable=protected-access\n        return _ProjectedTFXIO(\n            self.origin, self.projected._ProjectImpl(tensor_names)\n        )\n    return _ProjectedTFXIO(self, self._ProjectImpl(tensor_names))\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.RawRecordBeamSource","title":"RawRecordBeamSource","text":"<pre><code>RawRecordBeamSource() -&gt; PTransform\n</code></pre> <p>Returns a PTransform that produces a PCollection[bytes].</p> <p>Used together with RawRecordToRecordBatch(), it allows getting both the PCollection of the raw records and the PCollection of the RecordBatch from the same source. For example:</p> <p>record_batch = pipeline | tfxio.BeamSource() raw_record = pipeline | tfxio.RawRecordBeamSource()</p> <p>would result in the files being read twice, while the following would only read once:</p> <p>raw_record = pipeline | tfxio.RawRecordBeamSource() record_batch = raw_record | tfxio.RawRecordToRecordBatch()</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordBeamSource(self) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that produces a PCollection[bytes].\n\n    Used together with RawRecordToRecordBatch(), it allows getting both the\n    PCollection of the raw records and the PCollection of the RecordBatch from\n    the same source. For example:\n\n    record_batch = pipeline | tfxio.BeamSource()\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n\n    would result in the files being read twice, while the following would only\n    read once:\n\n    raw_record = pipeline | tfxio.RawRecordBeamSource()\n    record_batch = raw_record | tfxio.RawRecordToRecordBatch()\n    \"\"\"\n\n    @beam.typehints.with_input_types(Any)\n    @beam.typehints.with_output_types(bytes)\n    def _PTransformFn(pcoll_or_pipeline: Any):\n        return (\n            pcoll_or_pipeline\n            | \"ReadRawRecords\" &gt;&gt; self._RawRecordBeamSourceInternal()\n            | \"CollectRawRecordTelemetry\"\n            &gt;&gt; telemetry.ProfileRawRecords(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.RawRecordTensorFlowDataset","title":"RawRecordTensorFlowDataset","text":"<pre><code>RawRecordTensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Returns a Dataset that contains nested Datasets of raw records.</p> <p>May not be implemented for some TFXIOs.</p> <p>This should be used when RawTfRecordTFXIO.TensorFlowDataset does not suffice. Namely, if there is some logical grouping of files which we need to perform operations on, without applying the operation to each individual group (i.e. shuffle).</p> <p>The returned Dataset object is a dataset of datasets, where each nested dataset is a dataset of serialized records. When shuffle=False (default), the nested datasets are deterministically ordered. Each nested dataset can represent multiple files. The files are merged into one dataset if the files have the same format. For example:</p> <p><pre><code>file_patterns = ['file_1', 'file_2', 'dir_1/*']\nfile_formats = ['recordio', 'recordio', 'sstable']\ntfxio = SomeTFXIO(file_patterns, file_formats)\ndatasets = tfxio.RawRecordTensorFlowDataset(options)\n</code></pre> <code>datasets</code> would result in the following dataset: <code>[ds1, ds2]</code>. Where ds1 iterates over records from 'file_1' and 'file_2', and ds2 iterates over records from files matched by 'dir_1/*'.</p> <p>Example usage: <pre><code>tfxio = SomeTFXIO(file_patterns, file_formats)\nds = tfxio.RawRecordTensorFlowDataset(options=options)\nds = ds.flat_map(lambda x: x)\nrecords = list(ds.as_numpy_iterator())\n# iterating over `records` yields records from the each file in\n# `file_patterns`. See `tf.data.Dataset.list_files` for more information\n# about the order of files when expanding globs.\n</code></pre> Note that we need a flat_map, because <code>RawRecordTensorFlowDataset</code> returns a dataset of datasets.</p> <p>When shuffle=True, then the datasets not deterministically ordered, but the contents of each nested dataset are deterministcally ordered. For example, we may potentially have [ds2, ds1, ds3], where the contents of ds1, ds2, and ds3 are all deterministcally ordered.</p> <p>options: A TensorFlowDatasetOptions object. Not all options will apply.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordTensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Returns a Dataset that contains nested Datasets of raw records.\n\n    May not be implemented for some TFXIOs.\n\n    This should be used when RawTfRecordTFXIO.TensorFlowDataset does not\n    suffice. Namely, if there is some logical grouping of files which we need\n    to perform operations on, without applying the operation to each individual\n    group (i.e. shuffle).\n\n    The returned Dataset object is a dataset of datasets, where each nested\n    dataset is a dataset of serialized records. When shuffle=False (default),\n    the nested datasets are deterministically ordered. Each nested dataset can\n    represent multiple files. The files are merged into one dataset if the files\n    have the same format. For example:\n\n    ```\n    file_patterns = ['file_1', 'file_2', 'dir_1/*']\n    file_formats = ['recordio', 'recordio', 'sstable']\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    datasets = tfxio.RawRecordTensorFlowDataset(options)\n    ```\n    `datasets` would result in the following dataset: `[ds1, ds2]`. Where ds1\n    iterates over records from 'file_1' and 'file_2', and ds2 iterates over\n    records from files matched by 'dir_1/*'.\n\n    Example usage:\n    ```\n    tfxio = SomeTFXIO(file_patterns, file_formats)\n    ds = tfxio.RawRecordTensorFlowDataset(options=options)\n    ds = ds.flat_map(lambda x: x)\n    records = list(ds.as_numpy_iterator())\n    # iterating over `records` yields records from the each file in\n    # `file_patterns`. See `tf.data.Dataset.list_files` for more information\n    # about the order of files when expanding globs.\n    ```\n    Note that we need a flat_map, because `RawRecordTensorFlowDataset` returns\n    a dataset of datasets.\n\n    When shuffle=True, then the datasets not deterministically ordered,\n    but the contents of each nested dataset are deterministcally ordered.\n    For example, we may potentially have [ds2, ds1, ds3], where the\n    contents of ds1, ds2, and ds3 are all deterministcally ordered.\n\n    Args:\n    ----\n      options: A TensorFlowDatasetOptions object. Not all options will apply.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.RawRecordToRecordBatch","title":"RawRecordToRecordBatch","text":"<pre><code>RawRecordToRecordBatch(\n    batch_size: Optional[int] = None,\n) -&gt; PTransform\n</code></pre> <p>Returns a PTransform that converts raw records to Arrow RecordBatches.</p> <p>The input PCollection must be from self.RawRecordBeamSource() (also see the documentation for that method).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/record_based_tfxio.py</code> <pre><code>def RawRecordToRecordBatch(\n    self, batch_size: Optional[int] = None\n) -&gt; beam.PTransform:\n    \"\"\"Returns a PTransform that converts raw records to Arrow RecordBatches.\n\n    The input PCollection must be from self.RawRecordBeamSource() (also see\n    the documentation for that method).\n\n    Args:\n    ----\n      batch_size: if not None, the `pa.RecordBatch` produced will be of the\n        specified size. Otherwise it's automatically tuned by Beam.\n    \"\"\"\n\n    @beam.typehints.with_input_types(bytes)\n    @beam.typehints.with_output_types(pa.RecordBatch)\n    def _PTransformFn(pcoll: beam.pvalue.PCollection):\n        return (\n            pcoll\n            | \"RawRecordToRecordBatch\"\n            &gt;&gt; self._RawRecordToRecordBatchInternal(batch_size)\n            | \"CollectRecordBatchTelemetry\"\n            &gt;&gt; telemetry.ProfileRecordBatches(\n                self._telemetry_descriptors,\n                self._logical_format,\n                self._physical_format,\n            )\n        )\n\n    return beam.ptransform_fn(_PTransformFn)()\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.RecordBatches","title":"RecordBatches","text":"<pre><code>RecordBatches(options: RecordBatchesOptions)\n</code></pre> <p>Returns an iterable of record batches.</p> <p>This can be used outside of Apache Beam or TensorFlow to access data.</p> <p>options: An options object for iterating over record batches. Look at     <code>dataset_options.RecordBatchesOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def RecordBatches(self, options: dataset_options.RecordBatchesOptions):\n    raise NotImplementedError\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.SupportAttachingRawRecords","title":"SupportAttachingRawRecords","text":"<pre><code>SupportAttachingRawRecords() -&gt; bool\n</code></pre> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def SupportAttachingRawRecords(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter() -&gt; TensorAdapter\n</code></pre> <p>Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapter(self) -&gt; tensor_adapter.TensorAdapter:\n    \"\"\"Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n    return tensor_adapter.TensorAdapter(self.TensorAdapterConfig())\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig() -&gt; TensorAdapterConfig\n</code></pre> <p>Returns the config to initialize a <code>TensorAdapter</code>.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.TensorAdapterConfig--returns","title":"Returns","text":"<p>a <code>TensorAdapterConfig</code> that is the same as what is used to initialize the   <code>TensorAdapter</code> returned by <code>self.TensorAdapter()</code>.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapterConfig(self) -&gt; tensor_adapter.TensorAdapterConfig:\n    \"\"\"Returns the config to initialize a `TensorAdapter`.\n\n    Returns\n    -------\n      a `TensorAdapterConfig` that is the same as what is used to initialize the\n      `TensorAdapter` returned by `self.TensorAdapter()`.\n    \"\"\"\n    return tensor_adapter.TensorAdapterConfig(\n        self.ArrowSchema(), self.TensorRepresentations()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.TensorFlowDataset","title":"TensorFlowDataset","text":"<pre><code>TensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Creates a tf.data.Dataset that yields Tensors.</p> <p>The serialized tf.SequenceExamples are parsed by <code>tf.io.parse_sequence_example</code>.</p> <p>See base class (tfxio.TFXIO) for more details.</p> <p>options: an options object for the tf.data.Dataset. See     <code>dataset_options.TensorFlowDatasetOptions</code> for more details.</p> <p>A dataset of <code>dict</code> elements, (or a tuple of <code>dict</code> elements and label).   Each <code>dict</code> maps feature keys to <code>Tensor</code>, <code>SparseTensor</code>, or   <code>RaggedTensor</code> objects.</p> <p>ValueError: if there is something wrong with the provided schema.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def TensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Creates a tf.data.Dataset that yields Tensors.\n\n    The serialized tf.SequenceExamples are parsed by\n    `tf.io.parse_sequence_example`.\n\n    See base class (tfxio.TFXIO) for more details.\n\n    Args:\n    ----\n      options: an options object for the tf.data.Dataset. See\n        `dataset_options.TensorFlowDatasetOptions` for more details.\n\n    Returns:\n    -------\n      A dataset of `dict` elements, (or a tuple of `dict` elements and label).\n      Each `dict` maps feature keys to `Tensor`, `SparseTensor`, or\n      `RaggedTensor` objects.\n\n    Raises:\n    ------\n      ValueError: if there is something wrong with the provided schema.\n    \"\"\"\n    file_pattern = tf.convert_to_tensor(self._file_pattern)\n    dataset = dataset_util.make_tf_record_dataset(\n        file_pattern,\n        batch_size=options.batch_size,\n        num_epochs=options.num_epochs,\n        shuffle=options.shuffle,\n        shuffle_buffer_size=options.shuffle_buffer_size,\n        shuffle_seed=options.shuffle_seed,\n        reader_num_threads=options.reader_num_threads,\n        drop_final_batch=options.drop_final_batch,\n    )\n\n    return self._ParseRawRecordTensorFlowDataset(dataset, options.label_key)\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFSequenceExampleRecord.TensorRepresentations","title":"TensorRepresentations","text":"<pre><code>TensorRepresentations() -&gt; TensorRepresentations\n</code></pre> <p>Returns the <code>TensorRepresentations</code>.</p> <p>These <code>TensorRepresentation</code>s describe the tensors or composite tensors produced by the <code>TensorAdapter</code> created from <code>self.TensorAdapter()</code> or the tf.data.Dataset created from <code>self.TensorFlowDataset()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time. May raise an error if the tensor representations are invalid.</p> Source code in <code>tfx_bsl/tfxio/tf_sequence_example_record.py</code> <pre><code>def TensorRepresentations(self) -&gt; tensor_adapter.TensorRepresentations:\n    return tensor_representation_util.InferTensorRepresentationsFromMixedSchema(\n        self._schema\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO","title":"TFXIO","text":"<p>Abstract basic class of all TFXIO API implementations.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.ArrowSchema","title":"ArrowSchema  <code>abstractmethod</code>","text":"<pre><code>ArrowSchema() -&gt; Schema\n</code></pre> <p>Returns the schema of the <code>RecordBatch</code> produced by <code>self.BeamSource()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>@abc.abstractmethod\ndef ArrowSchema(self) -&gt; pa.Schema:\n    \"\"\"Returns the schema of the `RecordBatch` produced by `self.BeamSource()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.BeamSource","title":"BeamSource  <code>abstractmethod</code>","text":"<pre><code>BeamSource(batch_size: Optional[int] = None) -&gt; PTransform\n</code></pre> <p>Returns a beam <code>PTransform</code> that produces <code>PCollection[pa.RecordBatch]</code>.</p> <p>May NOT raise an error if the TFMD schema was not provided at construction time.</p> <p>If a TFMD schema was provided at construction time, all the <code>pa.RecordBatch</code>es in the result <code>PCollection</code> must be of the same schema returned by <code>self.ArrowSchema</code>. If a TFMD schema was not provided, the <code>pa.RecordBatch</code>es might not be of the same schema (they may contain different numbers of columns).</p> <p>batch_size: if not None, the <code>pa.RecordBatch</code> produced will be of the     specified size. Otherwise it's automatically tuned by Beam.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>@abc.abstractmethod\ndef BeamSource(self, batch_size: Optional[int] = None) -&gt; beam.PTransform:\n    \"\"\"Returns a beam `PTransform` that produces `PCollection[pa.RecordBatch]`.\n\n    May NOT raise an error if the TFMD schema was not provided at construction\n    time.\n\n    If a TFMD schema was provided at construction time, all the\n    `pa.RecordBatch`es in the result `PCollection` must be of the same schema\n    returned by `self.ArrowSchema`. If a TFMD schema was not provided, the\n    `pa.RecordBatch`es might not be of the same schema (they may contain\n    different numbers of columns).\n\n    Args:\n    ----\n      batch_size: if not None, the `pa.RecordBatch` produced will be of the\n        specified size. Otherwise it's automatically tuned by Beam.\n    \"\"\"\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.Project","title":"Project","text":"<pre><code>Project(tensor_names: List[str]) -&gt; TFXIO\n</code></pre> <p>Projects the dataset represented by this TFXIO.</p> <p>A Projected TFXIO: - Only columns needed for given tensor_names are guaranteed to be   produced by <code>self.BeamSource()</code> - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed   to contain only those tensors. - It retains a reference to the very original TFXIO, so its TensorAdapter   knows about the specs of the tensors that would be produced by the   original TensorAdapter. Also see <code>TensorAdapter.OriginalTensorSpec()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>tensor_names: a set of tensor names.</p> <p>A <code>TFXIO</code> instance that is the same as <code>self</code> except that:   - Only columns needed for given tensor_names are guaranteed to be     produced by <code>self.BeamSource()</code>   - <code>self.TensorAdapterConfig()</code> and <code>self.TensorFlowDataset()</code> are trimmed     to contain only those tensors.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def Project(self, tensor_names: List[str]) -&gt; \"TFXIO\":\n    \"\"\"Projects the dataset represented by this TFXIO.\n\n    A Projected TFXIO:\n    - Only columns needed for given tensor_names are guaranteed to be\n      produced by `self.BeamSource()`\n    - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n      to contain only those tensors.\n    - It retains a reference to the very original TFXIO, so its TensorAdapter\n      knows about the specs of the tensors that would be produced by the\n      original TensorAdapter. Also see `TensorAdapter.OriginalTensorSpec()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      tensor_names: a set of tensor names.\n\n    Returns:\n    -------\n      A `TFXIO` instance that is the same as `self` except that:\n      - Only columns needed for given tensor_names are guaranteed to be\n        produced by `self.BeamSource()`\n      - `self.TensorAdapterConfig()` and `self.TensorFlowDataset()` are trimmed\n        to contain only those tensors.\n    \"\"\"\n    if isinstance(self, _ProjectedTFXIO):\n        # pylint: disable=protected-access\n        return _ProjectedTFXIO(\n            self.origin, self.projected._ProjectImpl(tensor_names)\n        )\n    return _ProjectedTFXIO(self, self._ProjectImpl(tensor_names))\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.RecordBatches","title":"RecordBatches  <code>abstractmethod</code>","text":"<pre><code>RecordBatches(\n    options: RecordBatchesOptions,\n) -&gt; Iterator[RecordBatch]\n</code></pre> <p>Returns an iterable of record batches.</p> <p>This can be used outside of Apache Beam or TensorFlow to access data.</p> <p>options: An options object for iterating over record batches. Look at     <code>dataset_options.RecordBatchesOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>@abc.abstractmethod\ndef RecordBatches(\n    self, options: dataset_options.RecordBatchesOptions\n) -&gt; Iterator[pa.RecordBatch]:\n    \"\"\"Returns an iterable of record batches.\n\n    This can be used outside of Apache Beam or TensorFlow to access data.\n\n    Args:\n    ----\n      options: An options object for iterating over record batches. Look at\n        `dataset_options.RecordBatchesOptions` for more details.\n    \"\"\"\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter() -&gt; TensorAdapter\n</code></pre> <p>Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapter(self) -&gt; tensor_adapter.TensorAdapter:\n    \"\"\"Returns a TensorAdapter that converts pa.RecordBatch to TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    \"\"\"\n    return tensor_adapter.TensorAdapter(self.TensorAdapterConfig())\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig() -&gt; TensorAdapterConfig\n</code></pre> <p>Returns the config to initialize a <code>TensorAdapter</code>.</p>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.TensorAdapterConfig--returns","title":"Returns","text":"<p>a <code>TensorAdapterConfig</code> that is the same as what is used to initialize the   <code>TensorAdapter</code> returned by <code>self.TensorAdapter()</code>.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>def TensorAdapterConfig(self) -&gt; tensor_adapter.TensorAdapterConfig:\n    \"\"\"Returns the config to initialize a `TensorAdapter`.\n\n    Returns\n    -------\n      a `TensorAdapterConfig` that is the same as what is used to initialize the\n      `TensorAdapter` returned by `self.TensorAdapter()`.\n    \"\"\"\n    return tensor_adapter.TensorAdapterConfig(\n        self.ArrowSchema(), self.TensorRepresentations()\n    )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.TensorFlowDataset","title":"TensorFlowDataset  <code>abstractmethod</code>","text":"<pre><code>TensorFlowDataset(\n    options: TensorFlowDatasetOptions,\n) -&gt; Dataset\n</code></pre> <p>Returns a tf.data.Dataset of TF inputs.</p> <p>May raise an error if the TFMD schema was not provided at construction time.</p> <p>options: an options object for the tf.data.Dataset. Look at     <code>dataset_options.TensorFlowDatasetOptions</code> for more details.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>@abc.abstractmethod\ndef TensorFlowDataset(\n    self, options: dataset_options.TensorFlowDatasetOptions\n) -&gt; tf.data.Dataset:\n    \"\"\"Returns a tf.data.Dataset of TF inputs.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n\n    Args:\n    ----\n      options: an options object for the tf.data.Dataset. Look at\n        `dataset_options.TensorFlowDatasetOptions` for more details.\n    \"\"\"\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TFXIO.TensorRepresentations","title":"TensorRepresentations  <code>abstractmethod</code>","text":"<pre><code>TensorRepresentations() -&gt; TensorRepresentations\n</code></pre> <p>Returns the <code>TensorRepresentations</code>.</p> <p>These <code>TensorRepresentation</code>s describe the tensors or composite tensors produced by the <code>TensorAdapter</code> created from <code>self.TensorAdapter()</code> or the tf.data.Dataset created from <code>self.TensorFlowDataset()</code>.</p> <p>May raise an error if the TFMD schema was not provided at construction time. May raise an error if the tensor representations are invalid.</p> Source code in <code>tfx_bsl/tfxio/tfxio.py</code> <pre><code>@abc.abstractmethod\ndef TensorRepresentations(self) -&gt; tensor_adapter.TensorRepresentations:\n    \"\"\"Returns the `TensorRepresentations`.\n\n    These `TensorRepresentation`s describe the tensors or composite tensors\n    produced by the `TensorAdapter` created from `self.TensorAdapter()` or\n    the tf.data.Dataset created from `self.TensorFlowDataset()`.\n\n    May raise an error if the TFMD schema was not provided at construction time.\n    May raise an error if the tensor representations are invalid.\n    \"\"\"\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapter","title":"TensorAdapter","text":"<pre><code>TensorAdapter(config: TensorAdapterConfig)\n</code></pre> <p>A TensorAdapter converts a RecordBatch to a collection of TF Tensors.</p> <p>The conversion is determined by both the Arrow schema and the TensorRepresentations, which must be provided at the initialization time. Each TensorRepresentation contains the information needed to translates one or more columns in a RecordBatch of the given Arrow schema into a TF Tensor or CompositeTensor. They are contained in a Dict whose keys are the names of the tensors, which will be the keys of the Dict produced by ToBatchTensors().</p> <p>TypeSpecs() returns static TypeSpecs of those tensors by their names, i.e. if they have a shape, then the size of the first (batch) dimension is always unknown (None) because it depends on the size of the RecordBatch passed to ToBatchTensors().</p> <p>It is guaranteed that for any tensor_name in the given TensorRepresentations self.TypeSpecs()[tensor_name].is_compatible_with(     self.ToBatchedTensors(...)[tensor_name])</p> <p>Sliced RecordBatches and LargeListArray columns having null elements backed by non-empty sub-lists are not supported and will yield undefined behaviour.</p> Source code in <code>tfx_bsl/tfxio/tensor_adapter.py</code> <pre><code>def __init__(self, config: TensorAdapterConfig):\n    self._arrow_schema = config.arrow_schema\n    self._type_handlers = _BuildTypeHandlers(\n        config.tensor_representations, config.arrow_schema\n    )\n    self._type_specs = {\n        tensor_name: handler.type_spec\n        for tensor_name, handler in self._type_handlers\n    }\n\n    self._original_type_specs = (\n        self._type_specs\n        if config.original_type_specs is None\n        else config.original_type_specs\n    )\n\n    for tensor_name, type_spec in self._type_specs.items():\n        original_type_spec = self._original_type_specs.get(tensor_name, None)\n        if original_type_spec is None or original_type_spec != type_spec:\n            raise ValueError(\n                \"original_type_specs must be a superset of type_specs derived from \"\n                f\"TensorRepresentations. But for tensor {tensor_name}, got {original_type_spec} vs {type_spec}\"\n            )\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapter-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapter.OriginalTypeSpecs","title":"OriginalTypeSpecs","text":"<pre><code>OriginalTypeSpecs() -&gt; Dict[str, TypeSpec]\n</code></pre> <p>Returns the origin's type specs.</p> <p>A TFXIO 'Y' may be a result of projection of another TFXIO 'X', in which case then 'X' is the origin of 'Y'. And this method returns what X.TensorAdapter().TypeSpecs() would return.</p> <p>May equal to <code>self.TypeSpecs()</code>.</p> <p>Returns: a mapping from tensor names to <code>tf.TypeSpec</code>s.</p> Source code in <code>tfx_bsl/tfxio/tensor_adapter.py</code> <pre><code>def OriginalTypeSpecs(self) -&gt; Dict[str, tf.TypeSpec]:\n    \"\"\"Returns the origin's type specs.\n\n    A TFXIO 'Y' may be a result of projection of another TFXIO 'X', in which\n    case then 'X' is the origin of 'Y'. And this method returns what\n    X.TensorAdapter().TypeSpecs() would return.\n\n    May equal to `self.TypeSpecs()`.\n\n    Returns: a mapping from tensor names to `tf.TypeSpec`s.\n    \"\"\"\n    return self._original_type_specs\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapter.ToBatchTensors","title":"ToBatchTensors","text":"<pre><code>ToBatchTensors(\n    record_batch: RecordBatch,\n    produce_eager_tensors: Optional[bool] = None,\n) -&gt; Dict[str, Any]\n</code></pre> <p>Returns a batch of tensors translated from <code>record_batch</code>.</p> <p>record_batch: input RecordBatch.   produce_eager_tensors: controls whether the ToBatchTensors() produces     eager tensors or ndarrays (or Tensor value objects). If None, determine     that from whether TF Eager mode is enabled.</p> <p>RuntimeError: when Eager Tensors are requested but TF is not executing     eagerly.   ValueError: when Any handler failed to produce a Tensor.</p> Source code in <code>tfx_bsl/tfxio/tensor_adapter.py</code> <pre><code>def ToBatchTensors(\n    self, record_batch: pa.RecordBatch, produce_eager_tensors: Optional[bool] = None\n) -&gt; Dict[str, Any]:\n    \"\"\"Returns a batch of tensors translated from `record_batch`.\n\n    Args:\n    ----\n      record_batch: input RecordBatch.\n      produce_eager_tensors: controls whether the ToBatchTensors() produces\n        eager tensors or ndarrays (or Tensor value objects). If None, determine\n        that from whether TF Eager mode is enabled.\n\n    Raises:\n    ------\n      RuntimeError: when Eager Tensors are requested but TF is not executing\n        eagerly.\n      ValueError: when Any handler failed to produce a Tensor.\n    \"\"\"\n    tf_executing_eagerly = tf.executing_eagerly()\n    if produce_eager_tensors and not tf_executing_eagerly:\n        raise RuntimeError(\n            \"Eager Tensors were requested but eager mode was not enabled.\"\n        )\n    if produce_eager_tensors is None:\n        produce_eager_tensors = tf_executing_eagerly\n\n    if not record_batch.schema.equals(self._arrow_schema):\n        raise ValueError(\"Expected same schema.\")\n    result = {}\n    for tensor_name, handler in self._type_handlers:\n        try:\n            result[tensor_name] = handler.GetTensor(\n                record_batch, produce_eager_tensors\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Error raised when handling tensor '{tensor_name}'\"\n            ) from e\n\n    return result\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapter.TypeSpecs","title":"TypeSpecs","text":"<pre><code>TypeSpecs() -&gt; Dict[str, TypeSpec]\n</code></pre> <p>Returns the TypeSpec for each tensor.</p> Source code in <code>tfx_bsl/tfxio/tensor_adapter.py</code> <pre><code>def TypeSpecs(self) -&gt; Dict[str, tf.TypeSpec]:\n    \"\"\"Returns the TypeSpec for each tensor.\"\"\"\n    return self._type_specs\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapterConfig","title":"TensorAdapterConfig","text":"<pre><code>TensorAdapterConfig(\n    arrow_schema: Schema,\n    tensor_representations: TensorRepresentations,\n    original_type_specs: Optional[\n        Dict[str, TypeSpec]\n    ] = None,\n)\n</code></pre> <p>Config to a TensorAdapter.</p> <p>Contains all the information needed to create a TensorAdapter.</p> Source code in <code>tfx_bsl/tfxio/tensor_adapter.py</code> <pre><code>def __init__(\n    self,\n    arrow_schema: pa.Schema,\n    tensor_representations: TensorRepresentations,\n    original_type_specs: Optional[Dict[str, tf.TypeSpec]] = None,\n):\n    self.arrow_schema = arrow_schema\n    self.tensor_representations = tensor_representations\n    self.original_type_specs = original_type_specs\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapterConfig-attributes","title":"Attributes","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapterConfig.arrow_schema","title":"arrow_schema  <code>instance-attribute</code>","text":"<pre><code>arrow_schema = arrow_schema\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapterConfig.original_type_specs","title":"original_type_specs  <code>instance-attribute</code>","text":"<pre><code>original_type_specs = original_type_specs\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapterConfig.tensor_representations","title":"tensor_representations  <code>instance-attribute</code>","text":"<pre><code>tensor_representations = tensor_representations\n</code></pre>"},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorAdapterConfig-functions","title":"Functions","text":""},{"location":"api_docs/python/tfxio/#tfx_bsl.public.tfxio.TensorFlowDatasetOptions","title":"TensorFlowDatasetOptions","text":"<p>               Bases: <code>NamedTuple('TensorFlowDatasetOptions', [('batch_size', int), ('drop_final_batch', bool), ('num_epochs', Optional[int]), ('shuffle', bool), ('shuffle_buffer_size', int), ('shuffle_seed', Optional[int]), ('prefetch_buffer_size', int), ('reader_num_threads', int), ('parser_num_threads', int), ('sloppy_ordering', bool), ('label_key', Optional[str])])</code></p> <p>Options for TFXIO's TensorFlowDataset.</p> <p>Note: not all of these options may be effective. It depends on the particular TFXIO's implementation.</p>"}]}